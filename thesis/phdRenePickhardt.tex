%%%MAKE TODO COMMAND!

\documentclass[â€¢]{book}
\title{A study of generalized language models.}
\author{Rene Pickhardt (expecting xxx -yyy pages content without structure)}
\date{\today}
\begin{document}
\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%%	Introduction
%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\part{Foundations (do I need parts as a split level)}
%if there where parts I'd have:
%\begin{enumerate}
%\item basics (theory+emperical)
%\item working with GLM (application + indexing)
%\item summary future work etc...
%\end{enumerate}

\chapter{Introduction (x - y)}
this dissertation will mainly focus on \cite{own:typology:2013}
\section{Overview of the topic (x-y)}

\section{Running example (1 bis 2 pages)}
The core idea would be to choose an example or example setting of a person in his day life and show how he is confronted with all the various applications of generalized language models.
\begin{enumerate}
\item submitting text to a cellphone or search engine
\item handicaped people
\item speech recognition
\item text classification
\item WHAT ELSE?!? need to research applications
\end{enumerate}

\section{structure of the text (1-2)}

\section{Acknowledgements (1)}
\begin{itemize}
\item The community of the web providing this large mine of information. Especially the developers and everyone who publishes under an open licence and provides services. In particular the Wikimedia foundation and Stack Exchange. 
\item My students from whom I learnt most.
\item Steffen and Institute for creating and providing such a good environment
\item advisor Steffen, Thomas, Gerd.
\item Co authors for great discussion and good spirit (Thomas, Jonas, Till, Paul, Heinrich, Martin,)
\item readers of my blog (reading club, neo4j community, graph devroom)
\item External researchers from various conferences.\item friends
\item family and girlfriend

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%%	Let's try something
%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\intersection}{\cap}
\newcommand{\union}{\cup}
\newcommand{\skp}{\_}


\chapter{Mathematical foundations of Language Models (10 pages)}
Let $\Omega=\{w_1,...w_N\}$ be the set of all words in a language and $\Sigma$ be the $\sigma$-algebra of $\Omega$. 
A probability function $P:\Sigma \to [0,1]$ is a function such that.
\begin{enumerate}
  \item $P(\Omega) = 1$
  \item $P(\emptyset) = 0$
  \item $P(A_1 \cup A_2 \union \ldots \union A_n) = P(A_1) + P(A_2) + \ldots + P(A_n)$ \\
with $A_i \intersection A_j = \emptyset$ $\forall i,j \leq n, n < \infty$
\end{enumerate}
We call $(\Omega, \Sigma, P)$ a probability space. 
In future we will only have finit $\Omega$ and often fix $\Omega$ and $\Sigma$ but we will play around with $P$. 
Often we will take $\Omega = W := \{\texttt{Words within a fixed natural language}\}.$
Or we will take $\Omega = W^n$ the set of $n$-tuples or sequences of $n$ words over a given natural language. 

Sometimes we will also implicitly play around with $\Omega$ and $\Sigma$ without explicitly saying so and with similar looking formulars of $P$.
One of the resulting subtleties is pointed out in the following.
As all computer scientists when being in the natural language setting we will write by abuse of notation $P(w_i)$ when we mean $P(\{w_i\})$. 
We \textbf{must not confuse} $P(w_1 \union w_2) = P(\{w_1,w_2\})=P(\{w_1\}) + P(\{w_2\})$ by definition with $P'(w_1w_2):=P(\{(w_1,w_2)\})$ where $P'$ is defined on the $\sigma$-algebra of $W^2$ and P is defined on the $\sigma$-algebra of $W$.

Another example where abuse of notation leads to subtleties on $W^2$ is the following: We will often see something like $P'(w_2|w_1)$. 
Natural language experts wish to interpret this formula as the probability of a word $w_2$ occuring after a word $w_1$ has been seen.
For the sake of rigour we will once carry out the full argument of how this formula should be interpreted but in future stick to the abuse of notation. 
After the above discussion one might think that $P'(w_2|w_1)$ should read $P'(\{w_2\}|\{w_1\})$ which by definition of conditional probabilities is given by $\frac{P'(\{w_2\}\intersection \{w_1\})}{P'(\{w_1\})}$.
As we can see this does not make sense for the following two reasons.
First the result would always have to be $0$ as $P'(\{w_2\}\intersection \{w_1\}) = P'(\emptyset) = 0$ as long as $w_1\neq w_2$.
Second $\{w_1\}$ and $\{w_2\}$ are not subsets of $W^2$ and thus not elements of the $\sigma$-algebra of $W^2$ which means that $P'$ is not defined for the arguments we have been trying to pass to it so the expression does not carry any proper semantics if interpreted as above. 

A better interpretation is the following $P'(w_2|w_1):=\frac{P(w_1\skp \intersection \skp w_2)}{P'(w_1\skp)}$. 
In this case we define $w_1 \skp := \{(w_1,x)|x\in W\}\subset W^2$ and $\skp w_2 := \{(y,w_2)|y \in W\}\subset W^2$.
The intersection will be $P(w_1\skp \intersection \skp w_2) = P(\{(w_1,w_2)\})$ which is well defined. 
Also the probabilities $w_1 \skp$ and $\skp w_2$ are well defined since $W$ was assumed to be finite and we can create a disjoint union as we can see for $P(w_1 \skp) = P(\cup_{y\in W}\{(w_1,y)\}) = \sum_{y\in W}P(\{(w_1, y)\})$. $P(\skp w_2)$ will be calculated in an analogue way.


\section{Definition of a Language Model}
\section{Definition of an $n$-gram Model}
\section{The Chain rule of Probability for Marginal Probabilities}
\section{The Chain rule of Probability for Conditional Probabilties}
\section{Introduction of Maximum likelihood $n$-gram models}
\section{Messuring the quality of a language model}

\chapter{Mathematical foundations of Smoothing Methods (20 pages)}
\section{Why smoothing Language Models?}
\section{Averaging probability distributions}
\section{Laplace Smoothing}
\section{Absolute discounting}
\section{Backoff methods}
\section{Interpolated methods}
\section{Kneser Ney Smoothing}

\chapter{Problems with implementations of current methods (20 pages)}
As it turns out many of the above mentioned algorithms have been implemented incorrectly. 
A lot of the time the reason is improper notation which for example leads to the fact that marginal probabilities are used like conditional probabilities and vice verca. 

\section{Test of correctness}
All methods from the last chapter could be proven to be correct.
But when we look at implementations and language modelling toolkits we frequently find many problems since a lot of formulas seem ambigious. 
In these cases the implementations need to be checked for correctnes. 

One simple yet very effective test for correctness is testing if the sum over the probability of all events that can occure is actually 1. 
This is one reason why in a computer enviroment we will never talk about language models but $n$-gram models. 
Even over finite alphabets there are infinite elements in the set of all sentences. 
This means that doing this test for correct implementation is impossible for a computer.
\section{Marginals are not conditional probabilities}
\section{Marginals are not defined if the history was not seen}
\section{Markov assumption}
\section{Start and end of scentence tags}
\section{Problems with the unk token}

\chapter{Benchmark of Language model toolkits in terms of correctness (20 pages)}
as we saw in the last chapter a lot of times notation is arbitrary.
This means that we often have a choice. 
The first goal of this chapter is to test all the choices one can make for the well established methods in order to understand what is the best known language model. 
The second goal of this chapter is to test the existing implementations of the algorithms of correctness and be able to state which choices have been made by the toolkits.
\section{metrics for measuring - do we really need this?}
\section{used data sets}


\chapter{Generalized Language Models (20 pages)}
As we saw there exist good reasons to do backoff steps in language modelling. 
The question is why making the history one word shorter?
Another question is why actually making it shorter and not introducing a skip at the first position?
At this point one might wonder why not introducing skips at other positions? 
Introducing skips increases the probabilities. 
Maybe this is too much so we would like to introduce something that is not an arbitrary skip but also not a concrete word. Part of speeches seem to be reasonable though they could be replaced by any class leading to class based aproaches\cite{CLASS:BASED:LMS} or factored language models\cite{FACTORED:LANGUAGE:MODEL}.

Generalized Language models are a way to address these issues. 
They are fully compatible with the above mentioned smoothing methods. 

For the rest of this chapter we show how to define generalized language models and demonstrate their superiority in terms of perplexity. 
We will also demonstrate their downsides in terms of model parameter and storage space.


\section{Introductionary example, definition of the probem}
Here we would just do a maximum likelihood estimation of $n$-grams. 
Introduce one or two example sentences (which have to be very well choosen)
\section{Definition of skipped $n$-grams and differential notation}
\begin{enumerate}
\item notation. 
\item already have a motivation with some statistics taken from wikipedia or even put this earlier?
\end{enumerate}


\section{Generalized ARPA format for Generalized Language Models}
Also talking about the decoder
\section{API documentation to the GLM software toolkit}

\section{Pascal triangle of generalized language models}
how we can understand skipped $n$-grams and what our point of view should be

\section{Standard language models as a special case of generalized language models}



\chapter{Smoothing tequniques for generalized language models}
 
\section{kneser ney}
\section{laplace smoothing}
\section{other smoothing technique?}


\section{mathematical proof why generalized language models have to be better than language models}
only if I find one and if this is possible



\chapter{applications: next word prediction setting (5 pages)}
We will answer the question if generalized language models are only better in terms of perplexity or also outperforming in an application like next word prediction.
\section{recycle the old SIGIR submission and redo the experiments}

\chapter{applications: machine translation (5 pages)}
\section{Implementing GLM in KenLM}
\section{MOSES and basics of statistical Machine Translation}
\section{Results}
We will also aim to answer the question of performance in the setting of machine translation.


\chapter{Solving the Entropy Conjecture(10 - 15 pages)}
The conjecture states: Better results in Entropy of a LM will always positive correlate with better BLUE scores or Word error rates. The reason why this did not happen is that many implementations did not produce proper probability distributions.
\section{basics of speech recognition}
find co-author maybe steffen can help
\section{big comparison 3 methods: ASR, MT, NWP, vs various toolkits}


\chapter{Open ends (3 pages)}
\section{Pruning}
\section{Indexing}

first basic question: will indexing depend on the application ? I guess it does. but it sure makes sense to to be able to store generalized language models.
\subsection{filtering GLM to acertain size}
shall I make this?
\subsection{a data structure to store GLM}
also here no ideas and results available

\section{Distributed implementation}
\chapter{conclusion (2 pages)}



\chapter{Won't do Future Work (3 pages)}
\section{languagemodels for information retrieval}
together with thomas gottron. idea every document has its own langauge model... an let us measure Kullback-Leibler divergence
\subsection{semantic relatedness of words}
ESA work together with christoph schaefer

\bibliographystyle{plain}
\bibliography{phdRenePickhardt}

\end{document}
