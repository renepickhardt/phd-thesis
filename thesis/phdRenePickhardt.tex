\documentclass[â€¢]{book}
\title{A study of generalized language models.}
\author{Rene Pickhardt (expecting xxx -yyy pages content without structure)}
\date{\today}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{eqnarray}
\usepackage{mathtools}

% via: http://tug.ctan.org/macros/latex/contrib/glossaries/glossariesbegin.pdf
\usepackage[colorlinks]{hyperref}
\usepackage{glossaries}
\makenoidxglossaries


\newglossaryentry{os}{name={outcome space},description={A set - often denoted $\Omega$ - is called an outcome space}}
\newglossaryentry{powerset}{name={power set}, description={Given a finite or infinite set $\Omega$. The set of all subsets - denoted with $s^\Omega$ - is called the power set. For finite sets of $N$ elements it has $2^N$ elements}}

\newglossaryentry{ps}{name={probability space},description={A triplet $(\Omega, S, P)$ is called a probability space if $\Omega$ is an \gls{os}, $S$ is a \gls{sa} and $P$ is a \gls{pf}}}

\newglossaryentry{sa}{name={$\sigma$-algebra},
description={$S\subset 2^\Omega$ is called $\sigma$-algebra if the axioms of definition \ref{def:sa} hold}}
\newglossaryentry{pf}{name={probability function},
description={$P:S\longrightarrow [0,1]$ is called P probability function if $S$ is a \gls{sa} and the axioms of definition \ref{def:pf} hold}}

\newglossaryentry{lm}{name={language model},
description={\todo{rework! emphasize already in \gls{ps} that we usually write $P$ instead of the full triplet.}A triple $(\Omega,S,P)$ where $\Omega = W^{*}$ is the set of all sequences over a finite vocabulary $W$ and $S$ is a \gls{sa} over $\Omega$ and $P$ is a \gls{pf} over $S$}}

\newglossaryentry{ngm}{name={$n$-gram model},
description={A triple $(\Omega,S,P_n)$ where $\Omega = W^{n}$ is the set of all \glsplural{ngram} over a finite vocabulary $W$ and $S$ is a \gls{sa} over $\Omega$ and $P_n$ is a \gls{pf} over $S$ is called an $n$-gram model}}


\newglossaryentry{ngram}{name={$n$-gram},description={An $n$-gram is an element of $W^{n}$ given a finite vocabulary $W$. We denote a specific $n$-gram $w_1^n$ or $w_1\dots w_n$ instead of $(w_1,\dots,w_n)$}}
\newglossaryentry{sentence}{name={sentence}, description={A sentence is an element of $W^{*}$ given a finite vocabulary $W$ \todo{what about corpora}}}
\newglossaryentry{event}{name={event}, description={If $S$ is a \gls{sa} over $\Omega$ then subsets of $\Omega$ which are elements of $S$ are called events.}}

\newglossaryentry{mm}{name={markov model},
description={A Markov Model of order $n$ is a probability function $P(\cdot ; w_1\dots w_n):2^W\longrightarrow [0,1]$. It assigns probabilities to words occurring right after an n-gram $w_1\dots w_n$. this must not be confused with \glsplural{cp}}}
\newglossaryentry{fomm}{name={family of markov models},
description={A family of Markov Models is function $P_{W^n}:2^W\times W^{n}\longrightarrow [0,1]$ such that the derived functions $P_{W^n}(\cdot ; w_1^n)$ are a \glsplural{mm} for each $w_1^n\in W^{n}$}}
\newglossaryentry{mlm}{name={markov language model},
description={A language model $P$ is called a Markov language model of order $n$ if it is constructed with the help of families of Markov models up to order $n$ via the following construction: \\
$P(\{w_1^m\})=\lambda(m)\prod_{i=n+1}^mP_{W^n}(w_i;w_{i-n}^{i-1})\prod_{j=1}^nP_{W^j}(w_j;w_1^{j-1})$
}}
\newglossaryentry{nglm}{name={$n$-gram language model},
description={Given \glsplural{ngm} $\forall n\in\mathbb{N}$ then a language model $P$ is called an $n$-gram language model if for every $w_1^m \in W^{*}$ it is constructed via: \\
$P(\{w_1^m\})=\lambda(m)P_m(\{w_1^m\})$}}
\newglossaryentry{cp}{name={conditional probability},plural={conditional probabilities},
description={Given a \gls{pf} and two \glsplural{event} $A,B$ with $P(B)>0$. The conditional probability of $A$ given $B$ is defined as $P(A|B):=\frac{P(A\cap B)}{P(B)}$ it must not be confused with \glsplural{mm}}}

\newglossaryentry{mlem}{name={maximum likelihood model},
description={Given a \gls{corpus} $D$ and a \gls{cf} $c_n^D:W^{n}\longrightarrow\mathbb{N}_0$. Then \begin{align*}P_{W^n}(w_1^n):=\frac{c(w_1^n)}{\sum\limits_{s\in W^n}c_n^D(s)}\end{align*} is a maximum likelihood estimator of order $n$. It is an \gls{ngm}}}

\newglossaryentry{mmlem}{name={Markov maximum likelihood Model},
description={Given a \gls{corpus} $D$ and a \gls{cf} $c_{n+1}^D:W^{n+1}\longrightarrow\mathbb{N}_0$. If $\exists w_1^n \in W^{n},v\in W$ such that $c_{n+1}^D(w_1^nv) > 0$ Then \begin{align*}
P_{n}(w;w_1^n):=\frac{c_{n+1}^D(w_1^nw)}{\sum\limits_{v\in W}{c_{n+1}^D(w_1^nv)}}
\end{align*} is a Markov maximum likelihood model of order $n$.  It is a \gls{mlm} \todo{somewhere state that families of markov maximum likelihood models in practice will not exist and therefor a construction of a markov language model from a family of markov maximum likelihood models will most certainly}}}

\newglossaryentry{word}{name={word},
description={to come}}
\newglossaryentry{vocabulary}{name={vocabulary},
description={to come}}
\newglossaryentry{token}{name={token},
description={to come}}
\newglossaryentry{document}{name={document},
description={to come}}
\newglossaryentry{corpus}{name={corpus},
description={to comef}}

\newglossaryentry{cf}{name={counting function},
description={Given a \gls{corpus} D of text documents. $c_n^D:W^{n}\longrightarrow\mathbb{N}_0$ is called a counting function if it returns the absolut frequency how often each $n$-gram $w_1^n\in W^n$ occurs in $D$}}


\begin{document}
\maketitle
\tableofcontents



\newcommand{\todo}[1]{\textcolor{blue}{\textbf{TODO}: #1}}
\newcommand{\cexp}[1]{$\texttt{"#1"}$}
\makeatletter
\newcommand{\eqnum}{\refstepcounter{equation}\textup{\tagform@{\theequation}}}
\makeatother
\makeatletter
\newcommand{\eqlab}[1]{\refstepcounter{equation}\label{#1}\textup{\tagform@{\theequation}}}
\makeatother

% following: http://web.mat.bham.ac.uk/R.W.Kaye/latex/thm.pdf
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\newtheorem{example}{Example}[chapter]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%%	Introduction
%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\part{Foundations (do I need parts as a split level)}
%if there where parts I'd have:
%\begin{enumerate}
%\item basics (theory+emperical)
%\item working with GLM (application + indexing)
%\item summary future work etc...
%\end{enumerate}

\chapter{Introduction (x - y)}
this dissertation will mainly focus on \cite{own:typology:2013}
\section{Overview of the topic (x-y)}

\section{Running example (1 bis 2 pages)}
The core idea would be to choose an example or example setting of a person in his day life and show how he is confronted with all the various applications of generalized language models.
\begin{enumerate}
\item submitting text to a cellphone or search engine
\item Handicaped people
\item Speech recognition
\item Rext classification
\item Information retrieval
\item Spell checking
\item Machine Translation
\item WHAT ELSE?!? need to research applications
\end{enumerate}



\section{structure of the text (1-2)}

\section{Acknowledgements (1)}
\begin{itemize}
\item The community of creators on the web providing this large mine of information. Especially the developers and everyone who publishes under an open licence and provides services. In particular the Wikimedia foundation and Stack Exchange. 
\item My students from whom I learnt most. (Lukas, Sebastian, Michael, Carina, Web Science students)
\item Steffen and Institute for creating and providing such a good environment
\item advisor Steffen, Thomas, Gerd.
\item Co authors for great discussion and good spirit (Thomas, Jonas, Till, Paul, Heinrich, Martin,)
\item readers of my blog (reading club, neo4j community, graph devroom)
\item External researchers from various conferences.\item friends
\item family and girlfriend
\item In particular the tax payer of Germany who have founded my education as well as my research.
\item Klaus Dieter Schulz and friends

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%%	Let's try something
%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Mathematical foundations of Markov language models of order $n$ and $n$-gram language models}
In this chapter we will give an overview of the methods of statistical $n$-gram based language modeling and provide a mathematically rigorous and sound notation for language models. \todo{already mention terms like markov language model and $n$-gram language model?}
The later is needed since nowadays the notation in most publications is mathematically not sound and often oversimplified. Examples of this can be found in appendix \ref{a:ambigiousNotation}.
This might lead to ambiguous semantics for many of the formulas even for expert readers. 
The practitioner is left in doubt how to implement well known language modeling techniques correctly. 
We strongly suggest that the expert reader as well as the practitioner carefully study the following section and review our notation. 
The formulas and ideas of constructing Markov language models of order $n$ follow the standard text books and publications like \cite{chen:goodman}, \todo{add more bibliography} 
However the unification of the notation as well as our notion of what we call an $n$-gram language models (which we is different to what other authors mean by $n$-gram models) is our contribution and an integral part of this thesis. 

\section{Formal definitions of language models and $n$-gram models}\label{sec:basicDefinitions}
\begin{definition}
Let $W$ be a finite set such that $W = \{w_1,\dots,w_N\}$. The elements of $W$ are called \glsplural{word} and we call $W$ a \gls{vocabulary} $N$ words.
\end{definition}

\begin{definition}
Given a finite Vocabulary $W = \{w_1,\dots,w_N\}$ of $N$ words. 
We call $W^{n}=\underbrace{W\times \dots\times W}_{n-\texttt{times}}$ the set of sequences of length $n$ over the vocabulary $W$ or the set of \glsplural{ngram} over $W$. 
\end{definition}

\begin{definition}
$W^{*}$ is called the set of all sequences over $W$ or the set of \glsplural{sentence} over $W$.
\end{definition}

\begin{remark}
We state a couple of obvious observations and introduce some short forms to speed up notation while working with $n$-grams and sentences:
\begin{description}
\item[Size:] Obviously there are $N^n$ elements in the set of $n$-grams over $W$.
\begin{align}
\Leftrightarrow|W^{n}|=N^n
\end{align}
On the other hand the set of sequences over any vocabulary containing one or more words is infinite.
\begin{align}
\Leftrightarrow |W^{*}|=\infty
\end{align}
\item[Union:] We can understand $W^{*}$ as the countable but infinite union of sets of $n$-grams:
\begin{align}
W^{*}=\bigcup_{n=1}^\infty W^n
\end{align}
\item[Short forms] An element $(w_1,\dots,w_n) \in W^n$ will be written as $w_1\dots w_n$ or even shorter as $w_1^n$. For a sentence $s\in W^{*}$ we know $\exists ! m : s\in W^m$. We call $m$ the length of the sentence. In that case we will write $|s|=m$ or $len(s)=m$. Finally we often want to do operations on all words in a sentence. For example we might want to add something up for each word $w$ in $s$. By abuse of notation of the $\in$ operator we might write something like $\sum\limits_{w\in s}\dots$ in that case.
\end{description}
\end{remark}

\begin{definition}
A training \gls{document} $d$ is a series $(s_i)_{i=1\dots m}$ of $m$ sentences. We make the strong assumption that a word in a training corpus can be identified as something between two consecutive white space elements. 
\end{definition}

\begin{remark}
Since sentences are sequences of words e.g $s_1=w_1\dots w_x$ and $s_2=v_1\dots v_y$ we would run into the problem when we see something like $s_1s_2=w_1\dots w_xv_1\dots v_y$. 
Formally for the later there exist quite some combinations to draw two or more sentence borders.
However we do not wish to model punctuation marks into our vocabulary since these marks have special semantics and we would need to treat them with special caution within our calculations. 
Thus we assume that sentences have no punctuation marks at their end.
For a real training corpus we expect the corpus follows a format making sentence boundaries clear without punctuation marks.
For example we could have one sentence per line or the corpus comes as an XML-file where the sentences are enclosed by begin of sentence and end of sentence tags. 
\end{remark}

\begin{definition}
A training \gls{corpus} $D$ is a collection of documents. $D=\{d_1,\dots,d_M\}$
\end{definition}

\begin{remark}
Despite the fact that in many applications people wish to learn separate language models for separate documents in this thesis we are mainly interested learn a language model on a collection of sentences. We therefor assume - unless explicitly otherwise stated - that our training corpus consists of a single document. 
Further we will assume that the corpus comes in the format of one sentence per line without punctuation marks. 
\end{remark}

\begin{definition}
Given a \gls{corpus} D of text documents. $c_n^D:W^{n}\longrightarrow\mathbb{N}_0$ is called a counting function for $n$-grams if it returns the absolute frequency of how often each $n$ $w_1^n\in W^n$ occurs in all the sentences of $D$. In an $n$-gram is only counted if it stays within the sentence borders.
\end{definition}

\begin{example} A small text corpus $D$ is given by: \\
\texttt{a b b a c} \\
\texttt{a b c c b a} \\
\texttt{b b a c} \\
\texttt{c b a} \\
$D$ consists of one document with a vocabulary $W=\{\texttt{a,b,c}\}$ of three unique words building 4 sentences and a total of $18$ words. We have
\begin{align}
c_1^D(\texttt{a})=6 \hspace{1cm} c_1^D(\texttt{b})= 7 \hspace{1cm} c_1^D(\texttt{c})=5
\end{align}
with $\sum\limits_{w\in W}c_1^D(w) = 18$ the number of words in the corpus. We also have 
\begin{align*}
c_2^D(\texttt{a a})=0\hspace{1cm} c_2^D(\texttt{a b})=2\hspace{1cm}  c_2^D(\texttt{a c})=2 \\ 
c_2^D(\texttt{b a})=4\hspace{1cm} c_2^D(\texttt{b b})=2\hspace{1cm}  c_2^D(\texttt{b c})=1 \\
c_2^D(\texttt{c a})=0\hspace{1cm} c_2^D(\texttt{c b})=2\hspace{1cm}  c_2^D(\texttt{c c})=1 
\end{align*}
with $\sum\limits_{s\in W^2}c_2^D(w) = 14$ which is the number of words minus the number of sentences. \footnote{This will not directly generalize to some formula of counting how many $n$-grams exists. Though $|W| - (n-1)\times \texttt{\#sentences}$ seems tempting it already breaks in our example when counting the number of $5$-grams in this corpus or in general as soon as sentences of length shorter than $n$ exist. We provide this example to illustrate how we exactly count when doing implementations on a computer later on.} 
We also emphasize that $\sum_{w\in W}c_2^D(ww_i) \neq c_1^D(w_i)$ so for example $c_2^D(\texttt{a b}) + c_2^D(\texttt{b b})+ c_2^D(\texttt{c b}) = 2 + 2 + 2 = 6$ which is one less than $c_1^D(\texttt{b}) = 7$. This is quite clear since sentence $3$ starts with $\texttt{b}$ which leads to the fact that for one of the $7$ $\texttt{b}$'s in the corpus there is no bigram such that $\texttt{b}$ is the second letter. We only state this obvious observation this since in many text books including \cite{book:Jurafsky} it is falsely stated that $\sum_{w\in W}c_2^D(ww_i) = c_1^D(w_i)$ and that the reader should take a moment to be convinced of this.

Obviously real world corpora will most certainly consist of thousands of unique words building thousands or even millions of sentences and could easily have a billions words.
\end{example}


After having introduced terms specific for words and language we want to review some basic concepts of probability theory. 
We have limited ourselves to the notion of a probability function using $\sigma$-algebras as the concept of $\sigma$-additivity plays a central role in the process of constructing all kind of language models presented in this text.
However in order to stay as basic as possible for the non mathematician we omitted the notion of random variables as well as joint and marginal probability distributions.

\begin{definition}
Any set - usually denoted as $\Omega$ - can be called an \gls{os}. 
We call $2^\Omega:=\{A| A\subset \Omega\}$ the \gls{powerset} over $\Omega$.
\end{definition}
In this text a typical choice for $\Omega$ will be $W$,$W^n$ and $W^{*}$ for $n\in\mathbb{N}$.

\begin{definition}\label{def:sa}
Let $\Omega$ be an outcome space. 
A subset $S$ of $2^\Omega$ is called a $\sigma$-algebra over $\Omega$ iff:

\begin{description}
\item[Universal set:] $\Omega$ is in $S \Leftrightarrow \Omega \in S$
\item[Unions:] $S$ is closed under countable unions. 
\begin{align}
 \Leftrightarrow A_1,A_2,\dots \in S \Rightarrow \bigcup_{n\in\mathbb{N}}A_n \in S \label{eq:closedUnion}
\end{align}
\item[Complements:] $S$ is closed under complements
\begin{align}
\Leftrightarrow  A\in S \Rightarrow \Omega\backslash A \in S 
\end{align}
\end{description}
The elements of $S$ (which by definition are subsets of $\Omega$) are called \glsplural{event}.
In computer science literature $S$ is often called the event space.
\end{definition}


\begin{definition}\label{def:pf}
Let $S$ be a \gls{sa} over an \gls{os} $\Omega$. $P:S\longrightarrow [0,1]$ is called a \gls{pf} iff: 
$P$ respects the properties of a \gls{sa}. This means: 
\begin{description}
\item [$\sigma$-additivity:] $P$ is $\sigma$-additive which means for $A_1,A_2,\dots \in S$ with $A_i\cap A_j = \emptyset$ for all $i,j$ we have:
\begin{align}
P(\bigcup_iA_i) = \sum_iP(A_i)
\end{align}
\item[Summing up to 1:] $P(\emptyset)=0$ and $P(S)=1$. 
\end{description}
\end{definition}

\begin{definition}\label{def:ps}
A triplet $(\Omega,S,P)$ where $\Omega$ is and \gls{os} and $S$ is a \gls{sa} over $\Omega$ and $P$ is a \gls{pf} over $S$ is called a \gls{ps}.  
\end{definition}

\begin{remark}\label{rem:continuation}
Let $(\Omega,S,P)$ be a probability space. Let $S_1\subset {S}=\{s\in S : |s|=1\}$ be the set of all events in $S$ with a cardinality of $1$ in $\Omega$. Let as further assume $S_1$ generates $S$ via unions which means: $\forall s\in S \exists s_1,s_2,\dots \in S_1 : s = \bigcup_{i}s_i$. \footnote{Though this is a strong assumption it always holds if $S=2^\Omega$ which we assume throughout the text.} Knowing $P$ on $S_1$ is sufficient to know $P$ on all of $S$.
\begin{proof}
For an arbitrary $s\in S$ we can by assumption write:
\begin{align}
P(\{s\}) =  P(\bigcup_{i}\{s_i\}) = \sum_iP(\{s_i\})
\end{align}
We can use the $\sigma$-additivity in the last step since the sets $s_i,s_j$ contain just one element. Therefor they must be either pairwise disjoint or the same. Since the union of same elements will not change the result we can assume they are different and thus pairwise disjoint. 
\end{proof} 
Conversely, knowing $P$ on all events which have a cardinality of $1$ in $\Omega$ is particularly useful since these events are usually what is measured and what computer scientists work with.
\end{remark}


With this review of mathematical notation we can properly define the term \gls{lm} in consistence with the existing literature.

\begin{definition}\label{def:lm}
Let $\Omega = W^{*}$ be the set of \glsplural{sentence} over a finite vocabulary $W$ and $S$ be a \gls{sa} over $W^{*}$.
In that case a probability space $(W^{*}, S, P)$ is called a \gls{lm}.
\end{definition} 

\begin{remark}
As in the literature we might omit $\Omega$ and $S$ and only name the probability function $P$ when we want to talk about an entire probability space or a language model.
Though we try to minimize this behavior we emphasize that from the context it should always be clear what the spaces $\Omega$ and $S$ are.
\end{remark}

Having stated this the main question throughout this thesis (and the field of language modeling) reads:\\
\\
\textbf{Given a corpus of text documents, how can we estimate or learn a language model $P$ over the vocabulary that arises from that corpus?} 
\\
\\
%Since $\Omega^{*}$ is infinite even for small vocabularies it is a little bit tricky to handle so that most People will use $n$ gram models in order to estimate a language model (of order $n$).
Obviously it is not so clear how to define a probability function that assigns a probability for every sentence over a vocabulary. 
Since the set of all sentences is infinite we cannot use the uniform distribution (assuming each sentence is equally probable) since the uniform distribution is undefined over infinitely large probability spaces.  
One might argue that a sentence like \cexp{the the the the} is not a useful sentence and can be omitted. 
A counter argument is that the sentence just occurred in this thesis. 
But even the maximal subset of $W^{*}$ containing only plausible sentences is infinite.\footnote{I do not have a proof for this but it seems plausible when I see that James Joyce was able to put a sentence of 4,391 words into his novel Ulysses and techniques like recursion also exist for language. You could also read \url{https://en.wikipedia.org/w/index.php?title=Longest_English_sentence&oldid=736368728} for a discussion and further thoughts on this}
Therefor we can as well directly work with the general case of $W^{*}$.

\todo{maybe move to different place?} In order to tackle the problem of constructing a \gls{lm} people often make use of \glsplural{mm}, which will be defined later in this chapter, in order to construct \glsplural{mlm}. 
In addition now introduce our notion of \glsplural{ngm} which lead to our novel approach of creating what we call \glsplural{nglm}. 

\begin{definition}\label{def:ngram-model}We call the triplet $(W^{n}, S, P_n)$ an \gls{ngm} if $S$ is a $\sigma$-algebra over $W^{n}$ and $P_n:S\longrightarrow [0,1]$ is a probability function. We might omit the triplet and just say the $n$-gram model $P_n$. \footnote{$n$-gram models in the literature often have a form that looks like $P(w_n|w_1^{n-1})$ we refer to the appendix \ref{a:ambigiousNotation} to point out the problems with this definition. However we selected the same name for reasons that become clear throughout the text.}
\end{definition}

Before we introduce the Markov language models and the $n$-gram language models we want to make sense of the formal notation which was presented so far.
Therefor we will now give three very simple examples to construct $n$-gram models. If $n=1$ we will call them unigram models for $n=2$ we will call them bigram models and for $n=3$ we call them trigram models. 

\begin{example}\label{ex:unigram-model}
Given a corpus $D$ of text documents. Let $W$ be the set of unique words in $D$, with $|W|=N=\sum_{w\in W}c_1^D(w)$. Let $\Omega=W^n$ and $S=2^{\Omega}$. There are three well known ways of constructing $n$-gram models $(W^n,S,P_n)$. In particular for $n=1$ we call them unigram models. In all cases one uses the remark \ref{rem:continuation} during the construction. 
\begin{description}
\item[Uniform Model] We set $P_n^{uni}(\{w\})=\frac{1}{N^n}$ assigning all words an equal probability.  $P_n^{uni}$ is a probability function. 
\begin{proof}
\begin{align}
P_n^{uni}(\Omega) & = P_n^{uni}(\bigcup_{s\in\Omega}\{s\}) = \sum\limits_{s\in\Omega}P_n^{uni}(\{s\}) \label{eq:uniform-model}\\
 &  = \sum\limits_{s\in\Omega}\frac{1}{N^n}  =  \frac{1}{N^n} \sum\limits_{s\in\Omega}1= \frac{N^n}{N^n} = 1 
\end{align}
In \ref{eq:uniform-model} we first used remark \ref{rem:continuation} noting that $\Omega$ can be constructed from subsets containing just one element. 
Afterwards we used the $\sigma$-additivity which $P_n^{uni}$ should have by definition. 
\end{proof}

\item[Maximum Likelihood Estimator]  Let $c_n^D:\Omega = W^n\longrightarrow\mathbb{N}$ be the counting function \footnote{Even though we decided to omit the notion of random variables we emphasize for the experienced reader that $c_n^D$ is actually a random variable on the outcome space mapping elements of $\Omega$ to natural numbers. The trick in this and the following example will work with any random variable of this form.} We set: 
\begin{align}
P_n^{mle}(\{s\})=\frac{c_n^D(s)}{\sum\limits_{s'\in \Omega}c_n^D(s')}
\end{align}
$P_n^{mle}$ is a probability function:
\begin{proof}
Again we can first use remark \ref{rem:continuation} and $\sigma$-additivity to demonstrate that $P_n^{mle}$ will sum to $1$. 
\begin{align}
P_n^{mle}(\Omega) & = P_n^{mle}(\bigcup_{s\in\Omega}\{s\}) = \sum\limits_{s\in\Omega}P_n^{mle}(\{s\} )  \\
& =  \sum\limits_{s\in\Omega} \frac{c_n^D(s)}{\sum\limits_{s'\in \Omega}c_n^D(s')} = \frac{\sum\limits_{s\in \Omega}c_n^D(s)}{\sum\limits_{s'\in \Omega}c_n^D(s')} = 1
\end{align}
\end{proof}

\item[Continuation Count Estimator] We can create 
\begin{align}
cont_{n}^{D}:  \Omega = W^n & \rightarrow\mathbb{N} \\
w_1^n  & \mapsto |\{v| c_{n+1}^D(vw_1^n) > 0\}|
\end{align}
Similar to the previous example we can now set 
\begin{align}
P_n^{cont}(\{w\}) = \frac{cont_n^D(w_1^n)}{\sum\limits_{s'\in\Omega}cont_n^D(s')}
\end{align}
$P_n^{cont}$ is a probability function:
\begin{proof}
Again we make use of remark \ref{rem:continuation} and $\sigma$-additivity.
\begin{align}
P_n^{cont}(\Omega) & = P_n^{cont}(\bigcup_{s\in\Omega}\{s\}) = \sum_{s\in\Omega}P_n^{cont}(\{s\} )  \\
& =  \sum_{s\in\Omega} \frac{cont_n^D(s)}{\sum\limits_{s'\in \Omega}cont_n^D(s')} = \frac{\sum\limits_{s\in \Omega}cont_n^D(s))}{\sum\limits_{s'\in \Omega}cont_n^D(s')} = 1
\end{align} 
\end{proof}
Obviously we have: $cont_n^D(s) \leq c_n^D(s) \forall s\in \Omega$. Therefore it can happen that even though $c_n^D(s) \neq 0$ we have $P_n^{cont}(\{s\}) = 0$. This will be the case if $s$ only occurs at the beginning of sentences in $D$.
\end{description}
\end{example}

\section{Simple constructions of language models}
In this section we want to more closely examine the special case of $n=1$ and use the introduced unigram models from the last section (or any arbitrary known unigram model) in order to construct a language model. 
Later constructions in this text will follow the same principles. 
However we decided to first present the special case instead of the most general and abstract case in order to make this text more accessible for readers with fewer experience with mathematical literature.
We start by proofing a useful lemma that takes a central role in our constructions. 

\begin{lemma}\label{lem:sumProductProbabilitySpace}
Let $P_1$ be a unigram model over a vocabulary $W$. Then for all $n$ we have:
\[
\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\}) =1
\]
\end{lemma}
\begin{proof}
We proof this by induction.
\begin{description}
\item[Basis:] For $n=1$ the product contains only one term. So we have:
\[
\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\}) = \sum_{w\in W}P_1(\{w\}) = P_1(\bigcup_{w\in W}\{w\}) = P_1(W) =  1
\]

\item[inductive step:] Let us assume the lemma already holds for $m$. Let us see how we could make use of this in the case of $m+1$:
% \stackrel{\mathmakebox[\widthof{=}]\text{distributive law}}{=} http://tex.stackexchange.com/questions/217497/aligning-stackrel-signs-beneath-each-other-using-split
\begin{align}
 \sum_{s\in W^{m+1}}\prod_{w\in s}P_1(\{w\}) & = \sum_{w\in W}\left(P_1(\{w\})\underbrace{ \sum_{s\in W^m}\prod_{v\in s}P_1(\{v\}) } _{\text{=1 by assumption}}\right) \label{eq:lemSumProductProbabilitySpace}  \\
 &= \sum_{w\in W}P_1(\{w\}) = 1
\end{align}
Equation \ref{eq:lemSumProductProbabilitySpace} is true because of the distributive law.
The last step is true for the same reason as in the last steps of the basis.
Since the lemma is true for $n=1$ and we can deduce from $m$ to $m+1$ we can conclude the lemma it is true for all $n$.
\end{description}
\end{proof}


We can now proceed by constructing a language model.

\begin{theorem}\label{thm:unigram-lm}
Let $P_1$ be one of the unigram models defined in example \ref{ex:unigram-model} (or an arbitrary unigram model) and let $\rho \in (0,1)$.
For $s\in W^{*}$ with $s$ consisting of $m$ words and $m\geq 1$ we set
\begin{equation}
P(\{s\}) = \frac{\rho}{1-\rho}\prod_{w\in s}(1-\rho)P_1(\{w\}) =  \rho(1-\rho)^{m-1}\prod_{w\in s}P_1(\{w\})
\end{equation}
$P$ is a (unigram) language model over $W^{*}$.
\begin{proof}
We make use of remark \ref{rem:continuation}. 
Since the sets $\{s\}$ for $s\in W^{*}$ generate our $\sigma$-algebra $2^{W^{*}}$ and are pairwise disjoint we need to show that $P(W^{*})=1$ to check weather $P$ is a probability function 
If we manage to do so the proof is concluded.

\begin{description}
\item[First step] 
\begin{align}
P(W^n) & = P(\bigcup_{s\in W^n} \{s\}) = \sum_{s\in W^n}P(\{s\}) \\
 & = \sum_{s\in W^n}\rho(1-\rho)^{n-1}\prod_{w\in s}P_1(\{w\}) \\
 & = \rho(1-\rho)^{n-1} \underbrace{\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\})}_{\text{ = 1 according to lemma \ref{lem:sumProductProbabilitySpace}}} \\
 & = \rho(1-\rho)^{n-1}
\end{align}

\item[Second step] Since we can decompose $W^{*}$ into pairwise disjoint sets of sequences of fixed length we can make use of $\sigma$-additivity and thus have: 
\begin{align}
P(W^{*}) & = P(\bigcup_{n=1}^\infty W^n) = \sum_{n=1}^\infty P(W^n) = \sum_{n=1}^\infty \rho(1-\rho)^{n-1} \\
 & =  \rho \sum_{n=1}^\infty(1-\rho)^{n-1} = \rho \sum_{n=0}^\infty(1-\rho)^{n} \label{eq:geometricSeriesLHS} \\
 &  = \rho \frac{1}{1-(1-\rho)} = \frac{\rho}{\rho} = 1 \label{eq:geometricSeriesRHS}
\end{align}
We could got from \ref{eq:geometricSeriesLHS} to \ref{eq:geometricSeriesRHS} because we made use of the formula of the geometric series \footnote{c.f. the Wikipedia Article on the Geometric Series \url{https://en.wikipedia.org/w/index.php?title=Geometric_series&oldid=738893754} to see a proof. } and since $\rho$ was between $0$ and $1$.
\end{description} 
\end{proof}
\end{theorem}

\begin{remark}
The parameter $\rho$ is crucial for our proof to conclude.
If we had set $P'(\{s\}) = \prod_{w\in s}P_1(\{w\})$ we would not have been able to show that $P'(W^{*})=1$. 
In fact we would have had:
\begin{align}
P'(W^{*})= P(\bigcup_{i=1}^\infty W^n) = \sum_{i=1}^\infty P(W^n) = \sum_{i=1}^\infty 1 \label{tmp:eq:infty}
\end{align}
Since the result of equation \ref{tmp:eq:infty} is not finite we would not have had the chance to take its inverse as a normalizing factor in order to construct a probability function by force. 
There is also a nice proof by contradiction that $P'$ cannot be a language model which can be found in the appendix \ref{a:proofs}
\end{remark}

\begin{remark}
The parameter $\rho$ can be understood as the probability for a sentence to stop. 
It is an elegant way of expressing what many computer linguists are doing with begin of sentence tags like $\texttt{<BOS>}$ or $\texttt{<s>}$ and the corresponding end of sentence tags $\texttt{<EOS>}$ or $\texttt{</s>}$.
Having those one wonders if $\texttt{<s>}\in W$ and why it is forbidden to calculate $P(\texttt{</s><s>})$ which in the semantics of modeling language might not make sense but within the semantics of pure mathematics is a perfectly reasonable formula. \footnote{It is strange that in all papers and text books that I have looked at it is mentioned in a footnote that this trick with the tags makes language models some up to $1$ on the subset of $W^{*}$ in which sequences start with such a tag and end with the corresponding one. But only Ralf Schl\"uter from RWTH Aachen could show me an unpublished written proof of this fact in an oral discussion.} While these tokens might be a reasonable trick when implementing language models in software from a mathematical point of view they are disturbing.
\end{remark}

\begin{remark}
The way $\rho$ was used to encode the length distribution as a geometric distribution is consistent with what is being done in the literature (despite the fact that tokens like $\texttt{<s>}$ and $\texttt{</s>}$ are being used). This is in fact an oversimplification of empirical evidence showing that in reality the sentence length distribution is not a geometric distribution but rather a hypergeometric poisson distribution \footnote{c.f. \url{https://de.wikipedia.org/w/index.php?title=Gesetz_der_Verteilung_von_Satzl\%C3\%A4ngen&oldid=150830900}\todo{create some empirical plots on this}}.
\end{remark}

\begin{definition}
A function $\lambda:\mathbb{N}\longrightarrow [0,1]$ such that $\sum_{n=1}^\infty \lambda(n) = 1$ is called a sentence length distribution. It can easily be learned from any corpus.
\end{definition}

The just proofed theorem \ref{thm:unigram-lm} is a special case of the following more general theorem. 
Despite the fact that the proof follows exactly the same train of thoughts we followed it through once more since the argument is so central to this thesis.
\begin{theorem}\label{thm:unigramLMPlusSentenceLength}
Let $\lambda:\mathbb{N}\longrightarrow [0,1]$ be a sentence length distribution and $(W,2^W,P_1)$ a unigram model. For $s\in W^{*}$ with $s$ consisting of $m$ words and $m\geq 1$ we set
\begin{equation}
P(\{s\}) = \lambda(m)\prod_{w\in s}P_1(\{w\})
\end{equation}
The triplet $(W^{*},2^{W^{*}},P)$ is a (unigram) language model.
\begin{proof}
\begin{description}
\item[First step] 
\begin{align}
P(W^n) & = P(\bigcup_{s\in W_n} \{s\}) = \sum_{s\in W^n}P(\{s\}) \\
 & = \sum_{s\in W^n}\lambda(n)\prod_{w\in s}P_1(\{w\}) \\
 & = \lambda(n)\underbrace{\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\})}_{\text{ = 1 according to lemma \ref{lem:sumProductProbabilitySpace}}} \\
 & = \lambda(n)
\end{align}

\item[Second step] Since we can decompose $W^{*}$ into pairwise disjoint sets of sequences of fixed length we can make use of $\sigma$-additivity and thus have: 
\begin{align}
P(W^{*}) & = P(\bigcup_{n=1}^\infty W^n) = \sum_{n=1}^\infty P(W^n) = \sum_{n=1}^\infty \lambda(n) = 1 \\
\end{align} 
\end{description}
\end{proof}
\end{theorem}  

\begin{remark}
In theorem \ref{thm:unigram-lm} it was set to $\lambda(n) = \rho(1-\rho)^{n-1}$ following a geometric distribution. 
This situation is what seems to be implemented in most software toolkits and papers. \todo{double check and reference this}
\end{remark}



\section{Conditional Probabilities and Markov Assumption}
The careful reader will have realized that our definition of an $n$-gram model as a triple $(W^n,S,P_n)$ over a vocabulary $W$ is (purposely!) chosen differently to the definition of an $n$-gram model from the standard textbooks like \cite{book:Manning} or \cite{book:Jurafsky}. \todo{check bibliography}
In this section we want to explore the differences and thereby defend the rational for our naming convention. 

\subsection{Problem with conditional probabilities in the literature}
Within papers we frequently see something like: \textit{"We are interested in the probability $P$ of a word $w$ given a history $h$ of preceding words."}\todo{in the best case find a quote}
This is often stated as $P(w|h)$ or for a concrete example as $P(\texttt{Model}|\texttt{This is a Language})$. 
While most readers will have an intuition what is meant with this conditional probability from a mathematical point of view the semantics of the above formulas are not clear. \todo{totally unclear how deep I should dig. I have the feeling when staying so high level in my criticism that it might be defended. Opening up all the problems seems overwhelming. Maybe refering to the appendix?}
In these papers it is often omitted how $\Omega$ is chosen to be $W, W^n$ or $W^{*}$. 
More importantly the notation of a conditional probability is abused by this notation.
To illustrate this we recall the mathematical definition of a conditional probability.
\begin{definition}
Let $P$ be a probability function and $A,B$ to events such that $P(B)>0$.
\begin{align}
P(A|B):=\frac{P(A\cap B)}{P(B)}
\end{align}
is called the conditional probability of $P$
\end{definition}

Turning back to our example we see the resulting trouble right away. 
We have: 
\begin{align}
P(\texttt{Model}|\texttt{This is a Language}) = \frac{P(\texttt{Model}\cap\texttt{This is a Language})}{P(\texttt{This is a Language})}
\end{align}
This leaves one wondering what is meant by the intersection of a word and a sequence of words?
Even when assuming that the above formula should have looked like this
\begin{align}
P(\{\texttt{Model}\}|\{\texttt{This is a Language}\}) = \frac{P(\{\texttt{Model}\}\cap\{\texttt{This is a Language}\})}{P(\{\texttt{This is a Language}\})}
\end{align}
one realizes that the fraction only has a non empty intersection if $w$ is a subset of $h$ which is most certainly not the semantics that computer scientists wanted to express. 
\begin{remark}
Expert readers and critics might now argue that the literature at that point is implicitly introducing random variables 
\begin{align}
W_1,\dots W_n: W^{*}: \longrightarrow W
\end{align} where $W_i$ maps a sentence to its $i$-th word. 
In that case the meaning should rather be interpreted in the following way $P(\texttt{Model}|\texttt{This is a Language}) = P(W_5 = \texttt{Model}| W_1 = \texttt{This}, W_2=\texttt{is}, W_3 = \texttt{a}, W_4= \texttt{Language})$. 
While this in deed solves the problem at this stage one runs into severe problems as soon as we talk about the chain rule of total probability and the Markov assumption that is being made.
In that case by definition the resulting models will not sum up to $1$ making the perplexity measure which is used for evaluation completely meaningless. 
This again can be fixed by changing the semantics of the terms in the resulting product of the chain rule. We refer to the appendix \ref{a:ambigiousNotation} for a thorough discussion of these issues.
Introducing all this overhead and fixing the problems with the semantics of conditional probabilities leads to our following definition of Markov models. 
Therefore at this point we omit all this overhead and introduce these models directly.
\end{remark}

\subsection{Markov Assumption, History and Markov Models}
The constructed language models from theorem \ref{thm:unigram-lm} and \ref{thm:unigramLMPlusSentenceLength} have the problem that they will assign the most probable sequence of length $m$ as the sequence consisting of $m$ times the most probable word.
In such a language model trained on a real world corpus the sentence $\texttt{the the the the}$ would be more likely than $\texttt{I like language models}$ which would be an undesirable result. 
One way to tackle this problem is by changing the product formula of theorem \ref{thm:unigramLMPlusSentenceLength} so that its factors will become different probability functions for the probability of a word $w_i$ to occur at the $i$-the position in a resulting sentence.
One way is to make these probabilities depend on the last $n$ words that have been seen which is what people have been doing and what will be introduced now.

\begin{definition}
Let $S$ be a $\sigma$-algebra over $W$ for example $S=2^W$ and let $w_1^n\in W^n$ be a fixed $n$-gram.
We call the probability space $(W,2^W, P_1(\cdot ;w_1\dots w_n))$ a Markov model of order $n$. 
In particular we call $h=w_1\dots w_n = w_1^n$ the history.  
\end{definition}

\begin{remark}
$P_1(\cdot ; h)$ is basically a unigram model with outcome space $W$ and event space $2^W$. 
It has the parameters $h = w_1^n$.  
In that sense it can be seen exactly as probability function to predict a word $w$ following the history $h$ which formally was expressed with the help of conditional probabilities. 
\end{remark} 

Since at every position in a sentence we could not just have a fixed history $h$ but an arbitrary history we need to extend our definition.

\begin{definition}
Let $S$ be a $\sigma$-algebra over $W$.
We call
\begin{align}
P_1^n(\cdot ; \cdot): S\times W^{n}:\longrightarrow [0,1]
\end{align}
a \gls{fomm} of order $n$ if $\forall h\in W^n$ the associated function $P_1^n(\cdot ; h):S\longrightarrow [0,1]$ is a Markov model $(W,2^W,P_1(\cdot ; h))$.
\end{definition}
\begin{remark}
\begin{description}
\item[Markov Assumption] \todo{glossary} Having a family of Markov models of order $n$ means that we have introduced a Markov assumption of order $n$. For each $h$ of $n$ words we can interpret the outcome of the induced Markov model $P_1^n(\cdot ; h)$ as the probability function encoding the probability of a word to follow $h$. 
\item[Number of model parameters] Assuming a vocabulary with  $|W| = 100k = 10^5$ words. one sees that for $n=3$ one already has $10^{15}$ amount of word triples $h$. \todo{relate to petabytes or the number of words in the world wide web} 
In order to have a family of Markov Models of order $3$ for each of those a different histories a (separate) probability function has to be learned.
We call the number of probability functions that can be induced from a family of Markov models of order $n$ the number of model parameters. 
\item[Sparsity] We see that the task of learning a separate probability function for each model parameter will be quite challenging. With this huge amount of modeling parameters and the hapax legomenon property of language it is quite clear that even when taking a web corpus as our training data and when increasing the order of our Markov Assumption to $3$ or higher more than half of all possible histories might not have seen in the training data.
This makes it difficult to impractical to learn a unique probability function for each model parameter without introducing techniques like smoothing which will be explained in a later chapter.\footnote{Introducing smoothing will basically mean that the Markov assumption is weakened locally by decreasing its order at points where the training data is to sparse. In that sense we reduce the number of model parameters or at least do not have unique probability functions for each possible model parameter.}
\end{description}
\end{remark}

\begin{example}
Transfere the stuff from the previous section (uniform, MLE and contcounter to markov assumption (at least with order $1$ it should be possible.) Also discuss zero probabilities problem and well defined problem in this or in order $2$ setting. 
\end{example}
Despite the sparsity problems which will be fixed soon we can use families of conditioned unigram models of order $n$ or the markov assumption of order $n$ to construct another class of language models.  

\begin{lemma}\label{lem:constructLowerOrderMarkovFamilies}
Let $P_1^n$ be a family of markov models \todo{maybe this is a better name?} of order $n$. We can decrease the order by $1$ via the following process. 
For each $h=w_1\dots w_n\in W^n$ 
\begin{align}
P_1^{n-1}:(w;w_2\dots w_n):=\frac{1}{|W|}\sum_{v\in W}P_1^n(w;v w_2\dots w_n)
\end{align}
is a markov model or oder $n-1$ \todo{conflict how this process will not reflect BOS well which is why we need this process...}
\begin{proof}
Trivial! \todo{do it} Average of probability functions is a probability function. we just ddecreased the order by averaging over the first word (btw. there could have been other positions to average over and we arrive ad skip models. )
\end{proof}
\end{lemma}

like the unigram language model was constructed we can now construct a markov language model of order $n$ with the help of the lemma and we include the length distribution of sentences and copy the proof from above. \todo{rephrase sentence}
\begin{theorem}\label{thm:markovModelOrderN}
Let $P_1$ be a unigram Model, $P_1^1,\dots,P_1^n$ be $n$ families of of Markov Models of order $1$ to $n$ and $\lambda:\mathbb{N}\longrightarrow [0,1]$ a function such that $\sum_{i=1}^\infty\lambda(i) = 1$. For every $s\in W^{*}$ consisting of $m$ words such that $s=w_1^m$ \todo{introduce notation} and $m\geq 1$ we set
\begin{align}
P(\{s\})=\lambda(m)\prod_{i=n+1}^mP_1^n(w_i;w_{i-n}^{i-1})\prod_{j=1}^nP_1^{j}(w_j;w_1^{j-1})\label{formula:MarkovLMOrderN}
\end{align}
$P$ is a \gls{mlm} of order $n$.
\begin{proof}
The proof works exactly like theorem \ref{thm:unigram-lm} and \ref{thm:unigramLMPlusSentenceLength} using $\sigma$-additivity and the distributive law and is thus omitted. 
\end{proof}
\end{theorem}

\begin{remark}
The Language Models in the literature are exactly of this form.
However in the literature the motivation is completely different. 
It starts with conditional probabilities and the chain rule of total probability and then making a Markov assumption as well as incorporating begin of sentence and end of sentence tags. 

The second part of formula \ref{formula:MarkovLMOrderN} can be seen as the beginning of a sentence where one obviously doesn't have the preceeding $n$ words that would be needed to work with the Markov Model of order $n$. 

Though it is sufficient to have learnt / trained \todo{do I already want to use that terminology} one family of Markov Models of order $n$. 
The lower order families can be constructed with the help of lemma \ref{lem:constructLowerOrderMarkovFamilies}. 
It will be more accurate however - an most certainly lead to better results \footnote{we did not check this hypothesis} - if separate families are learnt just on the first $j$ words of the sentences in the corpora.  


\end{remark}

\todo{the interesting part is. All this is possible without smoothing and so on.}

\todo{emperical evaluation how a unigram model over all words is different to a unigram model which is only learnt on first words of a sentence}


\begin{itemize}
\item standard literature now uses conditional probabilities, the markov assumption and $n$-gram models (which are not the same as we defined) and extend the construction from theorem \ref{thm:unigram-lm}. This again is pretty ugly since we always need to switch cases of which models we use around begin and end of sentences. We would like to demonstrate this.
\item in particular discuss notion of chainrule and conditional probabilities
\end{itemize}

\section{Our approach of constructing higher order language models}
Having introduced the rigorous notation and having reviewed the well known techniques for constructing language models it is quite obvious that there is yet another way of constructing language models. 
This time we use $n$-gram models as defined in definition \ref{def:ngram-model} in section \ref{sec:basicDefinitions}. 
At a first glimpse the construction which we propose in this section seems to resemble the construction of Markov language models of order $n$. 
However we will give an example that illustrates the differences and supports the novelty of the following construction. \footnote{This construction was developed in May 2014 while having discussions with Heinrich Hartman in which we tried to give proper semantics to the notion of language models from the literature. A preliminary version of the discussion can be found in his git repository to which we both contributed at that time \todo{find link}}
\begin{theorem}\label{thm:higherOrderLM}
Let $\lambda:\mathbb{N}\longrightarrow [0,1]$ be a function such that $\sum_{i=1}^\infty\lambda(i) = 1$. 
For each $n\in\mathbb{N}$ let $P_n$ be an $n$-gram model as in definition \ref{def:ngram-model}. 
For every $s \in W^{*}$ with $s$ so consisting of $m$ words and $m\geq 1$ we set:
\begin{align}
P(\{s\}) = \lambda(m)P_m({s})
\end{align}
$P$ is an \gls{nglm} language model \todo{I need better names for the P's that arise from the three constructions}
\begin{proof}
As before we decompose $W^{*}$ into $\bigcup_{i=1}^\infty W^{i}$ so that we have:
\begin{align}
P(W^{*}) & = P(\bigcup_{i=1}^\infty W^{i}) =\sum_{i=1}^\infty P(W^{i})  \\
& =  \sum_{i=1}^\infty \lambda(i)\underbrace{P_i(W^{i})}_{\text{equal to 1}} = \sum_{i=1}^\infty \lambda(i) = 1
\end{align} 
\end{proof}
\end{theorem}

\begin{example}
Let $D$ be a corpus consisting of one sentence \texttt{a b a a b} over a vocabulary $W=\{\texttt{a},\texttt{b}\}$ consisting of two words.
Let $\lambda(n)=\rho(1-\rho)^{n-1}$ be a function used to construct the higher order language model and the Markov Model with $\rho \in (0,1)$. 
Let $c_n^D:W^n\longrightarrow \mathbb{N}_0$ be the counting function counting how often a certain $n$-gram occurs in our corpus $D$.
We can construct $n$-gram models in the following way:
\begin{align}
P_n(\{s\})=\frac{c_n^D(s)}{\sum_{\omega\in W^n}c_n^D(\omega)}
\end{align}
The $P_n$ give rise to a higher order language model $P$ as in theorem \ref{thm:higherOrderLM}.
For example we have $P(\{ab\})= \rho(1-\rho)\frac{2}{4}$ 

On the other hand we can construct families of Markov models of order $n$ by setting:
\begin{align}
P_1^n(w;w_1\dots w_n) = \frac{c_{n+1}^D(w_1\dots w_nw)}{\sum_{v\in W}c_{n+1}^D(w_1\dots w_nv)}
\end{align}
We emphasize here that one can already see a major drawback with the approach of Markov Models once a sequence $w_1\dots w_n$ was not seen in our corpus $D$ we have no unigram distribution for this combination of model parameters. \todo{somewhere discuss that higher order n-gram models also have many sparsities and at some point will also be undefined if $n$ is longer than the longest sentence in our corpus.}
Obviously smoothing or just selecting another distribution can help here but it must be done explicitly.
In particular there is a modeling choice to be made. 
Our example is chosen in a way such that we do not run into this problem. 

With the family of Markov Models we can construct Markov Language Models of order $n$ $P_n^{Markov}$ in the way as shown by theorem \ref{thm:markovModelOrderN}. 
In Particular we have a Markov Model of order $1$ which would be called a two-gram model in the common literature. 
We count:
\begin{align}
P_1^{Markov}(\{ab\}) & = \rho(1-\rho) P_1^1(\{b\};\{a\})P_1(\{b\}) = \rho(1-\rho) \frac{2}{3}\cdot\frac{2}{5} \\
 & = \rho(1-\rho)\frac{4}{15} \\
 & \neq  \underbrace{\rho(1-\rho) \frac{1}{2}}_{= P(\{ab\})}
\end{align}
\end{example}

The example shows that even when the same $\lambda$ was chosen and maximum likelihood estimators on $n$ grams the higher order language model and the Markov model would not coincide. 
This demonstrates the novelty of our construction of a language model. 

\begin{remark}
\begin{description}
\item[Sparcity and Choices] \todo{move thoughts that are within the example to here}
\item[Relation of Markov Models and Conditional Probabilities] conditional probabilities of $n$-gram models will be calculated exactly in the way how the markov models of order $n-1$ are defined. \todo{do an example but put this maybe to another position.}
\item[Dependence of Context] not only the markov models but also our higher order models depend on the last couple words and introduce this kind of dependency. \todo{discuss this further}
\end{description}
\end{remark}

\todo{maybe the above example would have already worked with uniform distributions for all choices leaving different results. though I guess that the results would be the same for uniform distributions since uniform distributions will not depend on the last $n$ words making the markov models obsolete.}
\todo{maybe emperically or theoretically show which model yields more data sparsity given the same amount of model parameters... this question is more of theoretical nature}

\begin{itemize}
\item Show how one can exchange the conditional probabilities and use the $n$-gram models that we have defined instead
\item recall my contributions to heinrichs git repository where I discussed how one could increase and decrease the order of $n$-gram models with the help of pullbacks on projection operators. 
\end{itemize}

\section{Conclusion and open questions}
We have seen three different constructions of language models the Unigram model as constructed in theorem \ref{thm:unigram-lm} and \ref{thm:unigramLMPlusSentenceLength}, the well known Markov language model of order $n$ as introduced in theorem \ref{thm:markovModelOrderN}, and our novel higher order language models as constructed in theorem \ref{thm:higherOrderLM}.
To be precise the Unigram Model can be seen as a Markov Model of order $0$.
\todo{Think about the following: To be precise a unigram model from Chapter 2 section 2 is a markov language model of order 0. I can also see them as 1-gram model. what about constructing a 2-gram language model by using a 2-gram model and multiplying all probabilities of 2-grams in the sentence. would this correspond to the markov model of order 1 or is this something completely different? Maybe my term $n$-gram language model is not so great after all?}
Therefore we have two substantially different constructions of Language Models which naturally leads to the following questions: 
\begin{enumerate}
\item Which of the two constructions is more accurate and yields better results in terms of perplexity but also in terms of applications?
\item Can the well known smoothing techniques for Markov models be transferred to our novel higher order Language Models (n-gram models)?
\item what are good methods of learning $n$-gram models?
\end{enumerate}
\todo{Actually we have Markov models and $n$-gram models. both can via a similar construction with the help of a sentence length distribution be used to construct a language model. We need good naming conventions for the components (Markov models and $n$-gram models, the families of those and the constructed Language models...)}

\todo{This is basically also what in the literature is being done with markov and chainrule. the factors in the product are just various unigram models parameterize by the last $n$ words and often written as $P(w_n|w_1^{n=1})$ these 'conditional' probabilities are constructed with the help on $n$-grams but can be seen as unigram models $P_1^{w_1^{n-1}}(\{w_n\})$}


\subsection{Roadmap:}
\begin{itemize}
\item introduce shorter notation for sequences like $w_1^n$
\item resolve slight notation inconsistencies from above. 
\end{itemize}

\chapter{Smoothing}
following the structure from the last chapter we can smooth $n$-gram models, markov families of order $n$ and any language model (most likely be smoothing the models which have been used to construct them. 

\begin{itemize}
\item discuss smoothing problems in all three cases.
\end{itemize}
\section{Markov Models}
\subsection{Laplace Smoothing}
\subsection{Interpolated Methods}
\subsection{Backing-off Methods}
\subsection{Kneser-Ney Smoothing}

\section{n-Gram Models}
\subsection{Laplace Smoothing}
\subsection{Interpolated Methods}
\subsection{Backing-off Methods}
\subsection{Kneser-Ney Smoothing}

\section{Maybe also Language Models}


\chapter{The case of generalized language models}
\todo{maybe this is only a section in the smoothing chapter}

\chapter{Emperically evaluating the quality of a language model}
Thoughts on perplexity, entropy, relative entropy, keystroke savings, next word predictions, etc... (other applications like IR, ASR, MT, SPELL CHECKING,) 

\section{Relative Entropy}
Perplexity and entropy values can not be compared directly since the maximal possible value depends on the number of elements in the probability space.
The highest value for entropy occurs with the uniform distribution. with $H(P^{uniform})=log(N)$ where $N = |\Omega|$. 
We therefor suggest to normalize all entropy values by dividing through the maximum entropy possible for probability space in question. 
\todo{the problem is on $W^{*}$ this is impossible. since the uniform distribution will not exist. However we can construct the relative entropy for unigram and $n$-gram models. Maybe we find a away how to use the values to know the relative entropy of our constructed lms...}

\chapter{Implementation hacks}
\section{Begin of Sentence tags}
maybe proof how begin of sentence tags translate to a certain kind of Markov Model. 
\section{The unknown word token - UNK}
Explain how to handle the unknown word token and why to best avoid it. 
\subsection{Fixing the vocabulary}

\chapter{empirical study, datasets, software toolkits, experiments.} 


\chapter{Related Work}
Discuss statistical language modeling as well as smoothing but also modern approaches like neural network based language models and deep learning. Also Baysian models should be mentioned here.

\chapter{Conclusions and Future Work}

\begin{appendix}
\chapter{Some proofs}\label{a:proofs}
Proofs which didn't make it to the main text.
\section*{Product Formula for Unigram Language Models}
\begin{theorem}
Let $P_1:2^W\longrightarrow[0,1]$ be a unigram model. For $s\in W^{*}$ set:
\begin{align}
P'(\{s\})= \prod_{w\in s}P_1(\{w\})
\end{align} 
Then $P'$ is not a language Model.
\end{theorem}
\begin{proof}
Let us assume $P'$ was a language model. In that case we have  $P'(W^{*}) =1$ by definition. On the other hand we have:
\begin{align}
P'(W) & = P'(\bigcup_{w\in W}\{w\})= \sum_{w\in W}P'(\{w\}) = \sum_{w\in W}P_1(\{w\}) \\
 & = P_1(\bigcup_{w\in W}\{w\})= P_1(W) = 1
\end{align}
Since $1 = P'(W^{*}) = P'(W^{*}\backslash W \cup W)= \underbrace{ P'(W^{*}\backslash W)}_{\text{must be }0} + \underbrace{â€¢P'(W)}_{=1}$  \\
From $P'(W^{*}\backslash W)=0$ it follows that $P(\{s\}) = 0 \forall s=(v,w)\in W^2$.\\
By definition we have 
\begin{align}
P'(\{s\})=P_1(\{v\})P_1(\{w\})  = 0 \forall v,w \in W
\end{align}
This can only be true if $P_1(\{w\}) = 0 \forall w$. That is an obvious contradiction to the assumption that $P_1(W)=1$. We conclude that our assumption that $P'$ was a probability function or a language model could not be true.
\end{proof}

\chapter{Some tables and graphics}
Empirical results which didn't make it to the main text.

\chapter{Examples of ambigious notation for Language Modeling from the Literature}\label{a:ambigiousNotation}

\end{appendix}

\glsaddall
\printnoidxglossaries

\bibliographystyle{plain}
\bibliography{phdRenePickhardt}

\end{document}
