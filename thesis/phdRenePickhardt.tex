\documentclass[•]{book}
\title{A study of generalized language models.}
\author{Rene Pickhardt (expecting xxx -yyy pages content without structure)}
\date{\today}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{eqnarray}
\usepackage{mathtools}

% via: http://tug.ctan.org/macros/latex/contrib/glossaries/glossariesbegin.pdf
\usepackage[colorlinks]{hyperref}
\usepackage{glossaries}
\makenoidxglossaries


\newglossaryentry{os}{name={outcome space},description={A set - often denoted $\Omega$ - is called an outcome space}}
\newglossaryentry{powerset}{name={power set}, description={Given a finite or infinite set $\Omega$. The set of all subsets - denoted with $s^\Omega$ - is called the power set. For finite sets of $N$ elements it has $2^N$ elements}}

\newglossaryentry{ps}{name={probability space},description={A triplet $(\Omega, S, P)$ is called a probability space if $\Omega$ is an \gls{os}, $S$ is a \gls{sa} and $P$ is a \gls{pf}}}

\newglossaryentry{sa}{name={$\sigma$-algebra},
description={$S\subset 2^\Omega$ is called $\sigma$-algebra if the axioms of definition \ref{def:sa} hold}}
\newglossaryentry{pf}{name={probability function},
description={$P:S\longrightarrow [0,1]$ is called P probability function if $S$ is a \gls{sa} and the axioms of definition \ref{def:pf} hold}}

\newglossaryentry{lm}{name={language model},
description={\todo{rework! emphasize already in \gls{ps} that we usually write $P$ instead of the full triplet.}A triple $(\Omega,S,P)$ where $\Omega = W^{*}$ is the set of all sequences over a finite vocabulary $W$ and $S$ is a \gls{sa} over $\Omega$ and $P$ is a \gls{pf} over $S$}}

\newglossaryentry{ngm}{name={$n$-gram model},
description={A triple $(\Omega,S,P_n)$ where $\Omega = W^{n}$ is the set of all \glsplural{ngram} over a finite vocabulary $W$ and $S$ is a \gls{sa} over $\Omega$ and $P_n$ is a \gls{pf} over $S$ is called an $n$-gram model}}


\newglossaryentry{ngram}{name={$n$-gram},description={An $n$-gram is an element of $W^{n}$ given a finite vocabulary $W$. We denote a specific $n$-gram $w_1^n$ or $w_1\dots w_n$ instead of $(w_1,\dots,w_n)$}}
\newglossaryentry{sentence}{name={sentence}, description={A sentence is an element of $W^{*}$ given a finite vocabulary $W$ \todo{what about corpora}}}
\newglossaryentry{event}{name={event}, description={If $S$ is a \gls{sa} over $\Omega$ then subsets of $\Omega$ which are elements of $S$ are called events.}}

\newglossaryentry{mm}{name={markov model},
description={A Markov Model of order $n$ is a probability function $P(\cdot ; w_1\dots w_n):2^W\longrightarrow [0,1]$. It assigns probabilities to words occurring right after an n-gram $w_1\dots w_n$. this must not be confused with \glsplural{cp}}}
\newglossaryentry{fomm}{name={family of markov models},
description={A family of Markov Models is function $P_{W^n}:2^W\times W^{n}\longrightarrow [0,1]$ such that the derived functions $P_{W^n}(\cdot ; w_1^n)$ are a \glsplural{mm} for each $w_1^n\in W^{n}$}}
\newglossaryentry{mlm}{name={markov language model},
description={A language model $P$ is called a Markov language model of order $n$ if it is constructed with the help of families of Markov models up to order $n$ via the following construction: \\
$P(\{w_1^m\})=\lambda(m)\prod_{i=n+1}^mP_{W^n}(w_i;w_{i-n}^{i-1})\prod_{j=1}^nP_{W^j}(w_j;w_1^{j-1})$
}}
\newglossaryentry{nglm}{name={$n$-gram language model},
description={Given \glsplural{ngm} $\forall n\in\mathbb{N}$ then a language model $P$ is called an $n$-gram language model if for every $w_1^m \in W^{*}$ it is constructed via: \\
$P(\{w_1^m\})=\lambda(m)P_m(\{w_1^m\})$}}
\newglossaryentry{cp}{name={conditional probability},plural={conditional probabilities},
description={Given a \gls{pf} and two \glsplural{event} $A,B$ with $P(B)>0$. The conditional probability of $A$ given $B$ is defined as $P(A|B):=\frac{P(A\cap B)}{P(B)}$ it must not be confused with \glsplural{mm}}}

\newglossaryentry{mlem}{name={maximum likelihood model},
description={Given a \gls{corpus} $D$ and a \gls{cf} $c_n^D:W^{n}\longrightarrow\mathbb{N}_0$. Then \begin{align*}P_{W^n}(w_1^n):=\frac{c(w_1^n)}{\sum\limits_{s\in W^n}c_n^D(s)}\end{align*} is a maximum likelihood estimator of order $n$. It is an \gls{ngm}}}

\newglossaryentry{mmlem}{name={Markov maximum likelihood Model},
description={Given a \gls{corpus} $D$ and a \gls{cf} $c_{n+1}^D:W^{n+1}\longrightarrow\mathbb{N}_0$. If $\exists w_1^n \in W^{n},v\in W$ such that $c_{n+1}^D(w_1^nv) > 0$ Then \begin{align*}
P_{n}(w;w_1^n):=\frac{c_{n+1}^D(w_1^nw)}{\sum\limits_{v\in W}{c_{n+1}^D(w_1^nv)}}
\end{align*} is a Markov maximum likelihood model of order $n$.  It is a \gls{mlm} \todo{somewhere state that families of markov maximum likelihood models in practice will not exist and therefor a construction of a markov language model from a family of markov maximum likelihood models will most certainly}}}

\newglossaryentry{word}{name={word},
description={to come}}
\newglossaryentry{vocabulary}{name={vocabulary},
description={to come}}
\newglossaryentry{token}{name={token},
description={to come}}
\newglossaryentry{document}{name={document},
description={to come}}
\newglossaryentry{corpus}{name={corpus},
description={to comef}}

\newglossaryentry{cf}{name={counting function},
description={Given a \gls{corpus} D of text documents. $c_n^D:W^{n}\longrightarrow\mathbb{N}_0$ is called a counting function if it returns the absolut frequency how often each $n$-gram $w_1^n\in W^n$ occurs in $D$}}


\begin{document}
\maketitle
\tableofcontents



\newcommand{\todo}[1]{\textcolor{blue}{\textbf{TODO}: #1}}
\newcommand{\cexp}[1]{$\texttt{"#1"}$}
\makeatletter
\newcommand{\eqnum}{\refstepcounter{equation}\textup{\tagform@{\theequation}}}
\makeatother
\makeatletter
\newcommand{\eqlab}[1]{\refstepcounter{equation}\label{#1}\textup{\tagform@{\theequation}}}
\makeatother

% following: http://web.mat.bham.ac.uk/R.W.Kaye/latex/thm.pdf
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\newtheorem{example}{Example}[chapter]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%%	Introduction
%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\part{Foundations (do I need parts as a split level)}
%if there where parts I'd have:
%\begin{enumerate}
%\item basics (theory+emperical)
%\item working with GLM (application + indexing)
%\item summary future work etc...
%\end{enumerate}

\chapter{Introduction (x - y)}
this dissertation will mainly focus on \cite{own:typology:2013}
\section{Overview of the topic (x-y)}

\section{Running example (1 bis 2 pages)}
The core idea would be to choose an example or example setting of a person in his day life and show how he is confronted with all the various applications of generalized language models.
\begin{enumerate}
\item submitting text to a cellphone or search engine
\item Handicaped people
\item Speech recognition
\item Rext classification
\item Information retrieval
\item Spell checking
\item Machine Translation
\item WHAT ELSE?!? need to research applications
\end{enumerate}



\section{structure of the text (1-2)}

\section{Acknowledgements (1)}
\begin{itemize}
\item The community of creators on the web providing this large mine of information. Especially the developers and everyone who publishes under an open licence and provides services. In particular the Wikimedia foundation and Stack Exchange. 
\item My students from whom I learnt most. (Lukas, Sebastian, Michael, Carina, Web Science students)
\item Steffen and Institute for creating and providing such a good environment
\item advisor Steffen, Thomas, Gerd.
\item Co authors for great discussion and good spirit (Thomas, Jonas, Till, Paul, Heinrich, Martin,)
\item readers of my blog (reading club, neo4j community, graph devroom)
\item External researchers from various conferences.\item friends
\item family and girlfriend
\item In particular the tax payer of Germany who have founded my education as well as my research.
\item Klaus Dieter Schulz and friends

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%%	Let's try something
%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Mathematical foundations of Markov language models of order $n$ and $n$-gram language models}
In this chapter we will give an overview of the methods of statistical $n$-gram based language modeling and provide a mathematically rigorous and sound notation for language models. \todo{already mention terms like markov language model and $n$-gram language model?}
The later is needed since nowadays the notation in most publications is mathematically not sound and often oversimplified. Examples of this can be found in appendix \ref{a:ambigiousNotation}.
This might lead to ambiguous semantics for many of the formulas even for expert readers. 
The practitioner is left in doubt how to implement well known language modeling techniques correctly. 
We strongly suggest that the expert reader as well as the practitioner carefully study the following section and review our notation. 
The formulas and ideas of constructing Markov language models of order $n$ follow the standard text books and publications like \cite{chen:goodman}, \todo{add more bibliography} 
However the unification of the notation as well as our notion of what we call an $n$-gram language models (which we is different to what other authors mean by $n$-gram models) is our contribution and an integral part of this thesis. 

\section{Formal definitions of Language Models, $n$-gram models and Markov models}\label{sec:basicDefinitions}
\begin{definition}
Let $W$ be a finite set such that $W = \{w_1,\dots,w_N\}$. The elements of $W$ are called \glsplural{word} and we call $W$ a \gls{vocabulary} $N$ words.
\end{definition}

\begin{definition}
Given a finite Vocabulary $W = \{w_1,\dots,w_N\}$ of $N$ words. 
We call $W^{n}=\underbrace{W\times \dots\times W}_{n-\texttt{times}}$ the set of sequences of length $n$ over the vocabulary $W$ or the set of \glsplural{ngram} over $W$. 
\end{definition}

\begin{definition}
$W^{*}$ is called the set of all sequences over $W$ or the set of \glsplural{sentence} over $W$.
\end{definition}

\begin{remark}
We state a couple of obvious observations and introduce some short forms to speed up notation while working with $n$-grams and sentences:
\begin{description}
\item[Size:] Obviously there are $N^n$ elements in the set of $n$-grams over $W$.
\begin{align}
\Leftrightarrow|W^{n}|=N^n
\end{align}
On the other hand the set of sequences over any vocabulary containing one or more words is infinite.
\begin{align}
\Leftrightarrow |W^{*}|=\infty
\end{align}
\item[Union:] We can understand $W^{*}$ as the countable but infinite union of sets of $n$-grams:
\begin{align}
W^{*}=\bigcup_{n=1}^\infty W^n
\end{align}
\item[Short forms] An element $(w_1,\dots,w_n) \in W^n$ will be written as $w_1\dots w_n$ or even shorter as $w_1^n$. For a sentence $s\in W^{*}$ we know $\exists ! m : s\in W^m$. We call $m$ the length of the sentence. In that case we will write $|s|=m$ or $len(s)=m$. Finally we often want to do operations on all words in a sentence. For example we might want to add something up for each word $w$ in $s$. By abuse of notation of the $\in$ operator we might write something like $\sum\limits_{w\in s}\dots$ in that case.
\end{description}
\end{remark}

After having introduced terms specific for words and language we want to review some basic concepts of probability theory. 
We have limited ourselves to the notion of a probability function using $\sigma$-algebras as the concept of $\sigma$-additivity plays a central role in the process of constructing all kind of language models presented in this text.
However in order to stay as basic as possible for the non mathematician we omitted the notion of random variables as well as joint and marginal probability distributions.

\begin{definition}
Any set - usually denoted as $\Omega$ - can be called an \gls{os}. 
We call $2^\Omega:=\{A| A\subset \Omega\}$ the \gls{powerset} over $\Omega$.
\end{definition}
In this text a typical choice for $\Omega$ will be $W$,$W^n$ and $W^{*}$ for $n\in\mathbb{N}$.

\begin{definition}\label{def:sa}
Let $\Omega$ be an outcome space. 
A subset $S$ of $2^\Omega$ is called a $\sigma$-algebra over $\Omega$ iff:

\begin{description}
\item[Universal set:] $\Omega$ is in $S \Leftrightarrow \Omega \in S$
\item[Unions:] $S$ is closed under countable unions. 
\begin{align}
 \Leftrightarrow A_1,A_2,\dots \in S \Rightarrow \bigcup_{n\in\mathbb{N}}A_n \in S \label{eq:closedUnion}
\end{align}
\item[Complements:] $S$ is closed under complements
\begin{align}
\Leftrightarrow  A\in S \Rightarrow \Omega\backslash A \in S 
\end{align}
\end{description}
The elements of $S$ (which by definition are subsets of $\Omega$) are called \glsplural{event}.
In computer science literature $S$ is often called the event space.
\end{definition}


\begin{definition}\label{def:pf}
Let $S$ be a \gls{sa} over an \gls{os} $\Omega$. $P:S\longrightarrow [0,1]$ is called a \gls{pf} iff: 
$P$ respects the properties of a \gls{sa}. This means: 
\begin{description}
\item [$\sigma$-additivity:] $P$ is $\sigma$-additive which means for $A_1,A_2,\dots in S$ and $\forall i,j$
\begin{align}
A_i\cap A_j = \emptyset \Rightarrow P(A_i\cup A_j) = P(A_i)+P(A_j)
\end{align}
\item[Summing up to 1:] $P(\emptyset)=0$ and $P(S)=1$. 
\end{description}
\end{definition}

\begin{definition}\label{def:ps}
A triplet $(\Omega,S,P)$ where $\Omega$ is and \gls{os} and $S$ is a \gls{sa} over $\Omega$ and $P$ is a \gls{pf} over $S$ is called a \gls{ps}.  
\end{definition}

With this review of mathematical notation we can properly define the term \gls{lm} in consistence with the existing literature.

\begin{definition}\label{def:lm}
Let $\Omega = W^{*}$ be the set of \glsplural{sentence} over a finite vocabulary $W$ and $S$ be a \gls{sa} over $W^{*}$.
In that case a probability space $(W^{*}, S, P)$ is called a \gls{lm}.
\end{definition} 

\begin{remark}
As in the literature we might omit $\Omega$ and $S$ and only name the probability function $P$ when we want to talk about an entire probability space or a language model.
Though we try to minimize this behavior we emphasize that from the context it should always be clear what the spaces $\Omega$ and $S$ are.
\end{remark}

Having stated this the main question throughout this thesis (and the field of language modeling) reads:\\
\\
\textbf{Given a corpus\todo{not defined yet} of text documents, how can we estimate or learn a language model $P$ over the vocabulary that arises from that corpus?} 
\\
\\
%Since $\Omega^{*}$ is infinite even for small vocabularies it is a little bit tricky to handle so that most People will use $n$ gram models in order to estimate a language model (of order $n$).
Obviously it is not so clear how to define a probability function that assigns a probability for every sentence over a vocabulary. 
Since the set of all sentences is infinite we cannot use the uniform distribution (assuming each sentence is equally probable) since the uniform distribution is undefined over infinitely large probability spaces.  
One might argue that a sentence like \cexp{the the the the} is not a useful sentence and can be omitted. 
A counter argument is that the sentence just occurred in this thesis. 
But even the maximal subset of $W^{*}$ containing only plausible sentences is infinite.\footnote{I do not have a proof for this but it seems plausible when I see that James Joyce was able to put a sentence of 4,391 words into his novel Ulysses and techniques like recursion also exist for language. You could also read \url{https://en.wikipedia.org/w/index.php?title=Longest_English_sentence&oldid=736368728} for a discussion and further thoughts on this}
Therefor we can as well directly work with the general case of $W^{*}$.

In order to tackle the problem of constructing a \gls{lm} people often make use of \glsplural{mm}, which will be defined later in this chapter, in order to construct \glsplural{mlm}. 
In addition we introduce our notion of \glsplural{ngm} which lead to our novel approach of creating what we call \glsplural{nglm}. 
Before we introduce all of that we want to make sense of the formal notation which was presented so far.
Therefor we will now give three very simple examples to construct language models. 

\begin{example}\label{ex:unigram-model}
Given a corpus $D$ of text documents. We will - here and in future - assume that $D$ contains of only one text without loss of generality. \todo{handle sentences} Let $\Omega=W$ be the set of unique words in $D$ and $N = |W|$ its cardinality and $S=2^{W}$. There are three well known ways of constructing a unigram model unigram model $(W,S,P_1)$. In all cases one uses the trick of example \ref{ex:continuation} in its construction. 
\begin{description}
\item[Uniform Model] We set $P_1^{uni}(\{w\})=\frac{1}{N}$ assigning all words an equal probability.  $P_1^{uni}$ is a probability function. 
\begin{proof}
\begin{align}
P_1^{uni}(\Omega) & = P_1^{uni}(\bigcup_{w\in\Omega}\{w\}) = \sum_{w\in\Omega}P_1^{uni}(\{w\}) \label{eq:uniform-model}\\
 &  = \sum_{w\in\Omega}\frac{1}{N}  =  \frac{1}{N} \sum_{w\in\Omega}1= \frac{N}{N} = 1 
\end{align}
In \ref{eq:uniform-model} we used the $\sigma$-additivity which $P_1^{uni}$ should have by definition. 
\end{proof}
\item[Maximum Likelihood Estimator]  Let $c_D:W\longrightarrow\mathbb{N}$ be  the counting function \footnote{actually it is a random variable - isn't it?} measuring  how often each word occurs we set: 
\begin{align}
P_1^{mle}(\{w\})=\frac{c_D(w)}{\sum_{v\in W}c_D(v)}
\end{align}. 
$P_1^{mle}$ is a probability function:
\begin{proof}
Again we can use $\sigma$-additivity to show that this will sum to $1$. 
\begin{align}
P_1^{mle}(\Omega) & = P_1^{mle}(\bigcup_{w\in\Omega}\{w\}) = \sum_{w\in\Omega}P_1^{mle}(\{w\} )  \\
& =  \sum_{w\in\Omega} \frac{c_D(w)}{\sum_{v\in W}c_D(v)} = \frac{\sum_{w\in \Omega}c_D(w)}{\sum_{v\in W}c_D(v)} = 1
\end{align}
\todo{be consistent with use of $W$ and $\Omega$}
\todo{c, cont and P have the n-gram count as sub and as super index this inconsistency should be resolved}.
\end{proof}
\item[Continuation Count Estimator] Let $W^2$ be the set of $2$-grams. 
Further the counting function $c_D^2:W^2\longrightarrow \mathbb{N}$ measures how often each $2$-gram occurs in $D$. 
We can create 
\begin{align}
cont_D:  W & \rightarrow\mathbb{N} \\
w  & \mapsto |\{v| c_D^2(v,w) > 0\}|
\end{align}
We can now set 
\begin{align}
P_1^{cont}(\{w\}) = \frac{cont_D(w)}{\sum_{v\in\Omega}cont_D(v)}
\end{align}
$P_1^{cont}$ is a probability function:
\begin{proof}
Again we use $\sigma$-additivity.
\begin{align}
P_1^{cont}(\Omega) & = P_1^{cont}(\bigcup_{w\in\Omega}\{w\}) = \sum_{w\in\Omega}P_1^{cont}(\{w\} )  \\
& =  \sum_{w\in\Omega} \frac{cont_D(w)}{\sum_{v\in W}cont_D(v)} = \frac{\sum_{w\in \Omega}cont_D(w))}{\sum_{v\in W}cont_D(v)} = 1
\end{align} 
\end{proof}
Obviously we have: $cont_D(w) \leq c_D(w) \forall w\in W$. Therefore it can happen that even though $c_D(w) \neq 0$ we have $P_1^{cont}(\{w\}) = 0$. This will be the case if $w$ only occurs at the beginning of documents in $D$.
 \todo{we have not specified the problem of sentences yet. This is particularly sad, since the entire notation makes BOS and EOS useless.}
\end{description}
\todo{we don't need c or cont just an arbitrary function can be used.}
\end{example}

\section{Constructing Language Models}

\begin{example}\label{ex:continuation}
Events in $S$ are subsets in $\Omega$ and have thus a cardinality.
If $S$ is generated by unions of events with a cardinality of $1$ it is sufficient to know the values of the \gls{pf} on the events with a cardinality of $1$. Because of the $\sigma$-additivity the continuation on all of $S$ is unique.  
Conversely, knowing $P$ on all events which have a cardinality of $1$ in $\Omega$ is particularly useful since these events are usually what is measured and what computer scientists work with.
\end{example}

\begin{definition}\label{def:ngram-model}We call the triple $(W^{n}, S, P_n)$ an \gls{ngm} if $S$ is a $\sigma$-algebra over $W^{n}$ and $P_n:S\longrightarrow [0,1]$ is a probability function. Often in the notation we omit the Triple and will just say the \gls{ngm} $P_n$. 
\end{definition}
\todo{discriminate against the classical known $n$-gram models which look rather like $P(w_n|w_1^{n-1})$}

We continue by proofing a useful lemma for the construction of language models. 

\begin{lemma}\label{lem:sumProductProbabilitySpace}
Let $P_1$ be a unigram model over a vocabulary $W$. Then for all $n$ we have:
\[
\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\}) =1
\]
\end{lemma}
\begin{proof}
We proof this by induction.
\begin{description}
\item[Basis:] For $n=1$ the product term contains only one term. So we have:
\[
\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\}) = \sum_{w\in W}P_1(\{w\}) = P_1(\bigcup_{w\in W}\{w\}) = P_1(W) =  1
\]

\item[inductive step:] Let us assume the lemma already holds for $m$. Let us see how we could make use for this in the case of $m+1$:
% \stackrel{\mathmakebox[\widthof{=}]\text{distributive law}}{=} http://tex.stackexchange.com/questions/217497/aligning-stackrel-signs-beneath-each-other-using-split
\begin{align}
 \sum_{s\in W^{m+1}}\prod_{w\in s}P_1(\{w\}) & = \sum_{w\in W}\left(P(\{w\})\underbrace{ \sum_{s\in W^m}\prod_{v\in s}P_1(\{v\}) } _{\text{=1 by assumption}}\right) \label{eq:lemSumProductProbabilitySpace}  \\
 &= \sum_{w\in W}P_1(\{w\}) = 1
\end{align}
Equation \ref{eq:lemSumProductProbabilitySpace} is true because of the distributive law.
The last step is true for the same reason as in the last steps of the basis.
Since the lemma is true for $n=1$ and we can deduce from $m$ to $m+1$ we can conclude the lemma it is true for all $n$.
\end{description}
\end{proof}


We can now proceed by constructing a language model.

\begin{theorem}\label{thm:unigram-lm}
Let $P_1$ be one of the unigram models defined in example \ref{ex:unigram-model} (or an arbitrary unigram model) and let $\rho \in (0,1)$.
For $s\in W^{*}$ with $s$ consisting of $m$ words and $m\geq 1$ we set
\begin{equation}
P(\{s\}) = \frac{\rho}{1-\rho}\prod_{w\in s}(1-\rho)P_1(\{w\}) =  \rho(1-\rho)^{m-1}\prod_{w\in s}P_1(\{w\})
\end{equation}
$P$ is a (unigram) language model over $W^{*}$.
\begin{proof}
Since the sets $\{s\}$ for $s\in W^{*}$ generate our $\sigma$-algebra $2^{W^{*}}$ and are pairwise disjoint we need to show that $P(W^{*})=1$ to check weather $P$ is a probability function and to conclude the proof.

\begin{description}
\item[First step] 
\begin{align}
P(W^n) & = P(\bigcup_{s\in W^n} \{s\}) = \sum_{s\in W^n}P(\{s\}) \\
 & = \sum_{s\in W^n}\rho(1-\rho)^{n-1}\prod_{w\in s}P_1(\{w\}) \\
 & = \rho(1-\rho)^{n-1} \underbrace{\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\})}_{\text{ = 1 according to lemma \ref{lem:sumProductProbabilitySpace}}} \\
 & = \rho(1-\rho)^{n-1}
\end{align}

\item[Second step] Since we can decompose $W^{*}$ into pairwise disjoint sets of sequences of fixed length we can make use of $\sigma$-additivity and thus have: 
\begin{align}
P(W^{*}) & = P(\bigcup_{n=1}^\infty W^n) = \sum_{n=1}^\infty P(W^n) = \sum_{n=1}^\infty \rho(1-\rho)^{n-1} \\
 & =  \rho \sum_{n=1}^\infty(1-\rho)^{n-1} = \rho \sum_{n=0}^\infty(1-\rho)^{n} \label{eq:geometricSeriesLHS} \\
 &  = \rho \frac{1}{1-(1-\rho)} = \frac{\rho}{\rho} = 1 \label{eq:geometricSeriesRHS}
\end{align}
We could got from \ref{eq:geometricSeriesLHS} to \ref{eq:geometricSeriesRHS} because we made use of the formula of the geometric series \footnote{c.f. the Wikipedia Article on the Geometric Series \url{https://en.wikipedia.org/w/index.php?title=Geometric_series&oldid=738893754} to see a proof. } and since $\rho$ was between $0$ and $1$.
\end{description} 
\end{proof}
\end{theorem}
\todo{This is basically also what in the literature is being done with markov and chainrule. the factors in the product are just various unigram models parameterize by the last $n$ words and often written as $P(w_n|w_1^{n=1})$ these 'conditional' probabilities are constructed with the help on $n$-grams but can be seen as unigram models $P_1^{w_1^{n-1}}(\{w_n\})$}
\begin{remark}
The parameter $\rho$ is crucial for our proof to conclude.
If we had set $P'(\{s\}) = \prod_{w\in s}P_1(\{w\})$ we would not have been able to show that $P'(W^{*})=1$. 
In fact we would have had:
\begin{align}
P'(W^{*})= P(\bigcup_{i=1}^\infty W^n) = \sum_{i=1}^\infty P(W^n) = \sum_{i=1}^\infty 1
\end{align}
Since the result is not finite we would not have had the chance to take its inverse as a normalizing factor. 
There is also a nice proof by contradiction that $P'$ cannot be a language model which can be found in the appendix \ref{a:proofs}

The parameter $\rho$ can be understood as the probability for a sentence to stop. 
It is thus an elegant way of expressing what the common literature is doing with begin of sentence tags and end of sentence tags.
In the standard literature having formulas like $P(\texttt{<BOS>})\left(=P(\texttt{<s>})\right)$ or $P(\texttt{<EOS>})\left(=P(\texttt{</s>})\right)$ \todo{missleading to have both formulations} lets one wonder why it is just forbidden to ask for $P(\texttt{</s><s>})$ which in the semantics of modeling language might not make sense but within the semantics of pure mathematics is a perfectly reasonable formula.
While these tokens might be a reasonable trick when implementing language models in software from a mathematical point of view they are disturbing.
In particular one does not need to add another token to the probability space in question - which again would results in strange behavior.   

The way $\rho$ was used to encode the length distribution as a geometric distribution is consistent with what is being done in the literature (despite the fact that tokens like $\texttt{<s>}$ and $\texttt{</s>}$ are being used). This is in fact an oversimplification but it is not reflecting empirical evidence that the sentence length distribution is not a geometric distribution but rather a hypergeometric poisson distribution \footnote{c.f. \url{https://de.wikipedia.org/w/index.php?title=Gesetz_der_Verteilung_von_Satzl\%C3\%A4ngen&oldid=150830900}\todo{create some empirical plots on this}}.

\end{remark}
\todo{technically $w\in s$ the in-operator is not well defined!}

The just proofed theorem \ref{thm:unigram-lm} is a special case of the following more general theorem. 
Despite the fact that the proof follows exactly the same train of thoughts we followed it through once more since the argument is so central to this thesis..
\begin{theorem}\label{thm:unigramLMPlusSentenceLength}
Let $\lambda:\mathbb{N}\longrightarrow [0,1]$ be a function such that $\sum_{n=1}^\infty \lambda(n) = 1$ then for $s\in W^{*}$ with $s$ consisting of $m$ words and $m\geq 1$ we set
\begin{equation}
P(\{s\}) = \lambda(m)\prod_{w\in s}P_1(\{w\})
\end{equation}
$P$ is a (unigram) language model over $W^{*}$.
\begin{proof}
\begin{description}
\item[First step] 
\begin{align}
P(W^n) & = P(\bigcup_{s\in W_n} \{s\}) = \sum_{s\in W^n}P(\{s\}) \\
 & = \sum_{s\in W^n}\lambda(n)\prod_{w\in s}P_1(\{w\}) \\
 & = \lambda(n)\underbrace{\sum_{s\in W^n}\prod_{w\in s}P_1(\{w\})}_{\text{ = 1 according to lemma \ref{lem:sumProductProbabilitySpace}}} \\
 & = \lambda(n)
\end{align}

\item[Second step] Since we can decompose $W^{*}$ into pairwise disjoint sets of sequences of fixed length we can make use of $\sigma$-additivity and thus have: 
\begin{align}
P(W^{*}) & = P(\bigcup_{n=1}^\infty W^n) = \sum_{n=1}^\infty P(W^n) = \sum_{n=1}^\infty \lambda(n) = 1 \\
\end{align} 
\end{description}
\end{proof}
\end{theorem}  

\begin{remark}
$\lambda$ can be seen as the sentence length distribution and should be learned from the empirical data.

In theorem \ref{thm:unigram-lm} it was set to $\lambda(n) = \rho(1-\rho)^{n-1}$ following a geometric distribution. 
This situation is what seems to be implemented in most software toolkits and papers. \todo{double check and reference this}
\end{remark}

\section{Semantics of Conditional Probabilities, Markov Assumption and number of model parameters}
The careful reader will have realized that our definition of an $n$-gram model as a triple $(W^n,S,P_n)$ over a vocabulary $W$ is different - and purposely meant to be different - to the definition from the standard literature.
In the following we want to explore the differences and thereby defend the rational for our naming convention. 

\subsection{Status quo}
Within papers we frequently see something like: \textit{We are interested in the probability $P$ of a word $w$ given a history $h$ of preceding words.}\todo{in the best case find a quote}
This is often stated as $P(w|h)$ or in our more concrete setting as $P(\texttt{Model}|\texttt{This is a Language})$. 
While most readers will have an intuition what is meant with this conditional probability from a mathematical point of view the semantics of the above formulas are not clear.
Not only is in the Papers omitted if $\Omega$ is chosen to be $W, W^n$ or $W^{*}$ but more important the notation of conditional probability is abused.
To illustrate this we recall the mathematical definition of a conditional probability.
\begin{definition}
Let $P$ be a probability function and $A,B$ to events such that $P(B)>0$.
\begin{align}
P(A|B):=\frac{P(A\cap B)}{P(B)}
\end{align}
is called the conditional probability of $P$
\end{definition}

Turning back to our example we see the resulting trouble right away. 
We have: 
\begin{align}
P(\texttt{Model}|\texttt{This is a Language}) = \frac{P(\texttt{Model}\cap\texttt{This is a Language})}{P(\texttt{This is a Language})}
\end{align}
This leaves one wondering what is meant by the intersection of a word and a sequence of words?
Even when assuming that the above formula should have looked like this
\begin{align}
P(\{\texttt{Model}\}|\{\texttt{This is a Language}\}) = \frac{P(\{\texttt{Model}\}\cap\{\texttt{This is a Language}\})}{P(\{\texttt{This is a Language}\})}
\end{align}
one realizes that the fraction only has a non empty intersection if $w$ is a subset of $h$ which is most certainly not the semantics that computer scientists wanted to express. 

\subsection{Markov Assumption}
We suggest in the case of words and language modeling to give another mathematical semantics to formulas looking like $P(w|h)$ which is reflecting the wish of computer scientists to express the probability of a word $w$ given the fact that it follows a history\todo{add to glossary} of words $h$. 

\begin{definition}
Let $S$ be a $\sigma$-algebra over $W$.
We call
\begin{align}
P_1^n(\cdot ; \cdot): S\times W^{n}:\longrightarrow [0,1]
\end{align}
a \gls{fomm} of order $n$ if $\forall h\in W^n$ the associated function $P_1^n(\cdot ; h):S\longrightarrow [0,1]$ is a unigram model over $W$. \todo{make sure index shifts are sensefull. it could also be $n-1$ or $n+1$}
\end{definition}
\begin{remark}
\begin{description}
\item[Markov Assumption] \todo{glossary AND leaving in remark?} Having a family of conditioned unigram models of order $n$ means that we have a Markov assumption of order $n$ since we can interpret the outcome of each unigram model for each combination $h$ of $n$ words as the probability distrubtion of a word following $h$. Thus the word $w$ is depending on the last $n$ words $h$
\item[Number of model parameters] Assuming a vocabulary with  $|W| = 100k = 10^5$ words. one sees that for $n=3$ one already has $10^{15}$ amount of word triples. \todo{relate to petabytes or the number of words in the world wide web} 
For each of those a different unigram model has to be learnt.
We see that this task might be quite difficult. This is true in particular because of the following.  
\item[Sparsity] With this huge amount of modeling parameters it is quite clear that even when taking a web corpus as our training data and when increasing the order of our Markov Assumption to $3$ or higher more than half of all possible combinations for our parameters might not have seen in the training data.
This makes it impossible to learn a unigram distribution for such model parameter combinations ins an obvious way without introducing techniques like smoothing which will be explained in a later chapter.
\end{description}
\end{remark}
\begin{example}
Transfere the stuff from the previous section (uniform, MLE and contcounter to markov assumption (at least with order $1$ it should be possible.) Also discuss zero probabilities problem and well defined problem in this or in order $2$ setting. 
\end{example}
Despite the sparsity problems which will be fixed soon we can use families of conditioned unigram models of order $n$ or the markov assumption of order $n$ to construct another class of language models.  

\begin{lemma}\label{lem:constructLowerOrderMarkovFamilies}
Let $P_1^n$ be a family of markov models \todo{maybe this is a better name?} of order $n$. We can decrease the order by $1$ via the following process. 
For each $h=w_1\dots w_n\in W^n$ 
\begin{align}
P_1^{n-1}:(w;w_2\dots w_n):=\frac{1}{|W|}\sum_{v\in W}P_1^n(w;v w_2\dots w_n)
\end{align}
is a markov model or oder $n-1$ \todo{conflict how this process will not reflect BOS well which is why we need this process...}
\begin{proof}
Trivial! \todo{do it} Average of probability functions is a probability function. we just ddecreased the order by averaging over the first word (btw. there could have been other positions to average over and we arrive ad skip models. )
\end{proof}
\end{lemma}

like the unigram language model was constructed we can now construct a markov language model of order $n$ with the help of the lemma and we include the length distribution of sentences and copy the proof from above. \todo{rephrase sentence}
\begin{theorem}\label{thm:markovModelOrderN}
Let $P_1$ be a unigram Model, $P_1^1,\dots,P_1^n$ be $n$ families of of Markov Models of order $1$ to $n$ and $\lambda:\mathbb{N}\longrightarrow [0,1]$ a function such that $\sum_{i=1}^\infty\lambda(i) = 1$. For every $s\in W^{*}$ consisting of $m$ words such that $s=w_1^m$ \todo{introduce notation} and $m\geq 1$ we set
\begin{align}
P(\{s\})=\lambda(m)\prod_{i=n+1}^mP_1^n(w_i;w_{i-n}^{i-1})\prod_{j=1}^nP_1^{j}(w_j;w_1^{j-1})\label{formula:MarkovLMOrderN}
\end{align}
$P$ is a \gls{mlm} of order $n$.
\begin{proof}
The proof works exactly like theorem \ref{thm:unigram-lm} and \ref{thm:unigramLMPlusSentenceLength} using $\sigma$-additivity and the distributive law and is thus omitted. 
\end{proof}
\end{theorem}

\begin{remark}
The Language Models in the literature are exactly of this form.
However in the literature the motivation is completely different. 
It starts with conditional probabilities and the chain rule of total probability and then making a Markov assumption as well as incorporating begin of sentence and end of sentence tags. 

The second part of formula \ref{formula:MarkovLMOrderN} can be seen as the beginning of a sentence where one obviously doesn't have the preceeding $n$ words that would be needed to work with the Markov Model of order $n$. 

Though it is sufficient to have learnt / trained \todo{do I already want to use that terminology} one family of Markov Models of order $n$. 
The lower order families can be constructed with the help of lemma \ref{lem:constructLowerOrderMarkovFamilies}. 
It will be more accurate however - an most certainly lead to better results \footnote{we did not check this hypothesis} - if separate families are learnt just on the first $j$ words of the sentences in the corpora.  


\end{remark}

\todo{the interesting part is. All this is possible without smoothing and so on.}

\todo{emperical evaluation how a unigram model over all words is different to a unigram model which is only learnt on first words of a sentence}


\begin{itemize}
\item standard literature now uses conditional probabilities, the markov assumption and $n$-gram models (which are not the same as we defined) and extend the construction from theorem \ref{thm:unigram-lm}. This again is pretty ugly since we always need to switch cases of which models we use around begin and end of sentences. We would like to demonstrate this.
\item in particular discuss notion of chainrule and conditional probabilities
\end{itemize}

\section{Our approach of constructing higher order language models}
Having introduced the rigorous notation and having reviewed the well known techniques for constructing language models it is quite obvious that there is yet another way of constructing language models. 
This time we use $n$-gram models as defined in definition \ref{def:ngram-model} in section \ref{sec:basicDefinitions}. 
At a first glimpse the construction which we propose in this section seems to resemble the construction of Markov language models of order $n$. 
However we will give an example that illustrates the differences and supports the novelty of the following construction. \footnote{This construction was developed in May 2014 while having discussions with Heinrich Hartman in which we tried to give proper semantics to the notion of language models from the literature. A preliminary version of the discussion can be found in his git repository to which we both contributed at that time \todo{find link}}
\begin{theorem}\label{thm:higherOrderLM}
Let $\lambda:\mathbb{N}\longrightarrow [0,1]$ be a function such that $\sum_{i=1}^\infty\lambda(i) = 1$. 
For each $n\in\mathbb{N}$ let $P_n$ be an $n$-gram model as in definition \ref{def:ngram-model}. 
For every $s \in W^{*}$ with $s$ so consisting of $m$ words and $m\geq 1$ we set:
\begin{align}
P(\{s\}) = \lambda(m)P_m({s})
\end{align}
$P$ is an \gls{nglm} language model \todo{I need better names for the P's that arise from the three constructions}
\begin{proof}
As before we decompose $W^{*}$ into $\bigcup_{i=1}^\infty W^{i}$ so that we have:
\begin{align}
P(W^{*}) & = P(\bigcup_{i=1}^\infty W^{i}) =\sum_{i=1}^\infty P(W^{i})  \\
& =  \sum_{i=1}^\infty \lambda(i)\underbrace{P_i(W^{i})}_{\text{equal to 1}} = \sum_{i=1}^\infty \lambda(i) = 1
\end{align} 
\end{proof}
\end{theorem}

\begin{example}
Let $D$ be a corpus consisting of one sentence \texttt{a b a a b} over a vocabulary $W=\{\texttt{a},\texttt{b}\}$ consisting of two words.
Let $\lambda(n)=\rho(1-\rho)^{n-1}$ be a function used to construct the higher order language model and the Markov Model with $\rho \in (0,1)$. 
Let $c_n^D:W^n\longrightarrow \mathbb{N}_0$ be the counting function counting how often a certain $n$-gram occurs in our corpus $D$.
We can construct $n$-gram models in the following way:
\begin{align}
P_n(\{s\})=\frac{c_n^D(s)}{\sum_{\omega\in W^n}c_n^D(\omega)}
\end{align}
The $P_n$ give rise to a higher order language model $P$ as in theorem \ref{thm:higherOrderLM}.
For example we have $P(\{ab\})= \rho(1-\rho)\frac{2}{4}$ 

On the other hand we can construct families of Markov models of order $n$ by setting:
\begin{align}
P_1^n(w;w_1\dots w_n) = \frac{c_{n+1}^D(w_1\dots w_nw)}{\sum_{v\in W}c_{n+1}^D(w_1\dots w_nv)}
\end{align}
We emphasize here that one can already see a major drawback with the approach of Markov Models once a sequence $w_1\dots w_n$ was not seen in our corpus $D$ we have no unigram distribution for this combination of model parameters. \todo{somewhere discuss that higher order n-gram models also have many sparsities and at some point will also be undefined if $n$ is longer than the longest sentence in our corpus.}
Obviously smoothing or just selecting another distribution can help here but it must be done explicitly.
In particular there is a modeling choice to be made. 
Our example is chosen in a way such that we do not run into this problem. 

With the family of Markov Models we can construct Markov Language Models of order $n$ $P_n^{Markov}$ in the way as shown by theorem \ref{thm:markovModelOrderN}. 
In Particular we have a Markov Model of order $1$ which would be called a two-gram model in the common literature. 
We count:
\begin{align}
P_1^{Markov}(\{ab\}) & = \rho(1-\rho) P_1^1(\{b\};\{a\})P_1(\{b\}) = \rho(1-\rho) \frac{2}{3}\cdot\frac{2}{5} \\
 & = \rho(1-\rho)\frac{4}{15} \\
 & \neq  \underbrace{\rho(1-\rho) \frac{1}{2}}_{= P(\{ab\})}
\end{align}
\end{example}

The example shows that even when the same $\lambda$ was chosen and maximum likelihood estimators on $n$ grams the higher order language model and the Markov model would not coincide. 
This demonstrates the novelty of our construction of a language model. 

\begin{remark}
\begin{description}
\item[Sparcity and Choices] \todo{move thoughts that are within the example to here}
\item[Relation of Markov Models and Conditional Probabilities] conditional probabilities of $n$-gram models will be calculated exactly in the way how the markov models of order $n-1$ are defined. \todo{do an example but put this maybe to another position.}
\item[Dependence of Context] not only the markov models but also our higher order models depend on the last couple words and introduce this kind of dependency. \todo{discuss this further}
\end{description}
\end{remark}

\todo{maybe the above example would have already worked with uniform distributions for all choices leaving different results. though I guess that the results would be the same for uniform distributions since uniform distributions will not depend on the last $n$ words making the markov models obsolete.}
\todo{maybe emperically or theoretically show which model yields more data sparsity given the same amount of model parameters... this question is more of theoretical nature}

\begin{itemize}
\item Show how one can exchange the conditional probabilities and use the $n$-gram models that we have defined instead
\item recall my contributions to heinrichs git repository where I discussed how one could increase and decrease the order of $n$-gram models with the help of pullbacks on projection operators. 
\end{itemize}

\section{Conclusion and open questions}
We have seen three different constructions of language models the Unigram model as constructed in theorem \ref{thm:unigram-lm} and \ref{thm:unigramLMPlusSentenceLength}, the well known Markov language model of order $n$ as introduced in theorem \ref{thm:markovModelOrderN}, and our novel higher order language models as constructed in theorem \ref{thm:higherOrderLM}.
To be precise the Unigram Model can be seen as a Markov Model of order $0$.
Therefore we have two substantially different constructions of Language Models which naturally leads to the following questions: 
\begin{enumerate}
\item Which of the two constructions is more accurate and yields better results in terms of perplexity but also in terms of applications?
\item Can the well known smoothing techniques for Markov models be transferred to our novel higher order Language Models (n-gram models)?
\item what are good methods of learning $n$-gram models?
\end{enumerate}
\todo{Actually we have Markov models and $n$-gram models. both can via a similar construction with the help of a sentence length distribution be used to construct a language model. We need good naming conventions for the components (Markov models and $n$-gram models, the families of those and the constructed Language models...)}




\subsection{Roadmap:}
\begin{itemize}
\item introduce shorter notation for sequences like $w_1^n$
\item resolve slight notation inconsistencies from above. 
\end{itemize}

\chapter{Smoothing}
following the structure from the last chapter we can smooth $n$-gram models, markov families of order $n$ and any language model (most likely be smoothing the models which have been used to construct them. 

\begin{itemize}
\item discuss smoothing problems in all three cases.
\end{itemize}
\section{Markov Models}
\subsection{Laplace Smoothing}
\subsection{Interpolated Methods}
\subsection{Backing-off Methods}
\subsection{Kneser-Ney Smoothing}

\section{n-Gram Models}
\subsection{Laplace Smoothing}
\subsection{Interpolated Methods}
\subsection{Backing-off Methods}
\subsection{Kneser-Ney Smoothing}

\section{Maybe also Language Models}


\chapter{The case of generalized language models}
\todo{maybe this is only a section in the smoothing chapter}

\chapter{Emperically evaluating the quality of a language model}
Thoughts on perplexity, entropy, relative entropy, keystroke savings, next word predictions, etc... (other applications like IR, ASR, MT, SPELL CHECKING,) 

\section{Relative Entropy}
Perplexity and entropy values can not be compared directly since the maximal possible value depends on the number of elements in the probability space.
The highest value for entropy occurs with the uniform distribution. with $H(P^{uniform})=log(N)$ where $N = |\Omega|$. 
We therefor suggest to normalize all entropy values by dividing through the maximum entropy possible for probability space in question. 
\todo{the problem is on $W^{*}$ this is impossible. since the uniform distribution will not exist. However we can construct the relative entropy for unigram and $n$-gram models. Maybe we find a away how to use the values to know the relative entropy of our constructed lms...}

\chapter{Implementation hacks}
\section{Begin of Sentence tags}
maybe proof how begin of sentence tags translate to a certain kind of Markov Model. 
\section{The unknown word token - UNK}
Explain how to handle the unknown word token and why to best avoid it. 
\subsection{Fixing the vocabulary}

\chapter{empirical study, datasets, software toolkits, experiments.} 


\chapter{Related Work}
Discuss statistical language modeling as well as smoothing but also modern approaches like neural network based language models and deep learning. Also Baysian models should be mentioned here.

\chapter{Conclusions and Future Work}

\begin{appendix}
\chapter{Some proofs}\label{a:proofs}
Proofs which didn't make it to the main text.
\section*{Product Formula for Unigram Language Models}
\begin{theorem}
Let $P_1:2^W\longrightarrow[0,1]$ be a unigram model. For $s\in W^{*}$ set:
\begin{align}
P'(\{s\})= \prod_{w\in s}P_1(\{w\})
\end{align} 
Then $P'$ is not a language Model.
\end{theorem}
\begin{proof}
Let us assume $P'$ was a language model. In that case we have  $P'(W^{*}) =1$ by definition. On the other hand we have:
\begin{align}
P'(W) & = P'(\bigcup_{w\in W}\{w\})= \sum_{w\in W}P'(\{w\}) = \sum_{w\in W}P_1(\{w\}) \\
 & = P_1(\bigcup_{w\in W}\{w\})= P_1(W) = 1
\end{align}
Since $1 = P'(W^{*}) = P'(W^{*}\backslash W \cup W)= \underbrace{ P'(W^{*}\backslash W)}_{\text{must be }0} + \underbrace{•P'(W)}_{=1}$  \\
From $P'(W^{*}\backslash W)=0$ it follows that $P(\{s\}) = 0 \forall s=(v,w)\in W^2$.\\
By definition we have 
\begin{align}
P'(\{s\})=P_1(\{v\})P_1(\{w\})  = 0 \forall v,w \in W
\end{align}
This can only be true if $P_1(\{w\}) = 0 \forall w$. That is an obvious contradiction to the assumption that $P_1(W)=1$. We conclude that our assumption that $P'$ was a probability function or a language model could not be true.
\end{proof}

\chapter{Some tables and graphics}
Empirical results which didn't make it to the main text.

\chapter{Examples of ambigious notation for Language Modeling from the Literature}\label{a:ambigiousNotation}

\end{appendix}

\glsaddall
\printnoidxglossaries

\bibliographystyle{plain}
\bibliography{phdRenePickhardt}

\end{document}
