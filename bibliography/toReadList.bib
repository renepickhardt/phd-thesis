% This file was created with JabRef 2.7b.
% Encoding: ISO8859_1

@INPROCEEDINGS{Brants07largelanguage,
  author = {Thorsten Brants and Ashok C. Popat and Peng Xu and Franz J. Och and
	Jeffrey Dean and Google Inc},
  title = {Large language models in machine translation},
  booktitle = {In EMNLP},
  year = {2007},
  pages = {858--867},
  file = {:2007LargeLanguageModelsInMachineTranslation.pdf:PDF}
}

@ARTICLE{Chelba2013,
  author = {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and
	Thorsten Brants and Phillipp Koehn and Tony Robinson},
  title = {One Billion Word Benchmark for Measuring Progress in Statistical
	Language Modeling},
  year = {2013},
  month = dec,
  __markedentry = {[rpickhardt:]},
  abstract = {We propose a new benchmark corpus to be used for measuring progress
	in statistical language modeling. With almost one billion words of
	training data, we hope this benchmark will be useful to quickly evaluate
	novel language modeling techniques, and to compare their contribution
	when combined with other advanced techniques. We show performance
	of several well-known types of language models, with the best results
	achieved with a recurrent neural network based language model. The
	baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6;
	a combination of techniques leads to 35% reduction in perplexity,
	or 10% reduction in cross-entropy (bits), over that baseline. The
	benchmark is available as a code.google.com project; besides the
	scripts needed to rebuild the training/held-out data, it also makes
	available log-probability values for each word in each of ten held-out
	data sets, for each of the baseline n-gram models.},
  comments = {Accompanied by a code.google.com project allowing anyone to generate
	the benchmark data, and use it to compare their language model against
	the ones described in the paper},
  eprint = {1312.3005},
  file = {:oneBillionWordBenchmarkFormeasuringProgressInStatisticalLanguageModeling2014.pdf:PDF},
  oai2identifier = {1312.3005},
  owner = {rpickhardt},
  timestamp = {2014.05.06}
}

@ARTICLE{goodman2001bit,
  author = {Goodman, Joshua},
  title = {A bit of progress in language modeling},
  journal = {Computer Speech \& Language},
  year = {2001},
  volume = {15},
  pages = {403--434},
  number = {4},
  file = {:goodman2001bit.pdf:PDF},
  owner = {rpickhardt},
  publisher = {Elsevier},
  timestamp = {2014.01.10}
}

@INCOLLECTION{RW:FST:2001,
  author = {Mihov, Stoyan and Maurel, Denis},
  title = {Direct construction of minimal acyclic subsequential transducers},
  booktitle = {Implementation and Application of Automata},
  publisher = {Springer},
  year = {2001},
  pages = {217--229},
  owner = {rpickhardt},
  timestamp = {2014.05.10}
}

@INPROCEEDINGS{Talbot07smoothedbloom,
  author = {David Talbot and Miles Osborne},
  title = {Smoothed bloom filter language models: Tera-scale LMs on the cheap},
  booktitle = {In Proc. of ACL},
  year = {2007},
  pages = {468--476},
  file = {:Talbot2007smoothedbloom.pdf:PDF},
  owner = {rpickhardt},
  timestamp = {2014.05.10}
}

