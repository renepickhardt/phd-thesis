% This file was created with JabRef 2.7b.
% Encoding: ISO8859_1

@INPROCEEDINGS{Brants07largelanguage,
  author = {Thorsten Brants and Ashok C. Popat and Peng Xu and Franz J. Och and
	Jeffrey Dean and Google Inc},
  title = {Large language models in machine translation},
  booktitle = {In EMNLP},
  year = {2007},
  pages = {858--867},
  file = {:2007LargeLanguageModelsInMachineTranslation.pdf:PDF}
}

@ARTICLE{J:CL:1990:BrownSMP,
  author = {Brown, Peter F. and deSouza, Peter V. and Mercer, Robert L. and Pietra,
	Vincent J. Della and Lai, Jenifer C.},
  title = {Class-based N-gram Models of Natural Language},
  journal = {Comput. Linguist.},
  year = {1992},
  volume = {18},
  pages = {467--479},
  number = {4},
  month = dec,
  acmid = {176316},
  address = {Cambridge, MA, USA},
  file = {:J\:CL\:1990\:BrownSMP.pdf:PDF},
  issn = {0891-2017},
  issue_date = {December 1992},
  numpages = {13},
  owner = {rpickhardt},
  publisher = {MIT Press},
  timestamp = {2014.05.15},
  url = {http://dl.acm.org/citation.cfm?id=176313.176316}
}

@ARTICLE{Chelba2013,
  author = {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and
	Thorsten Brants and Phillipp Koehn and Tony Robinson},
  title = {One Billion Word Benchmark for Measuring Progress in Statistical
	Language Modeling},
  year = {2013},
  month = dec,
  __markedentry = {[rpickhardt:]},
  abstract = {We propose a new benchmark corpus to be used for measuring progress
	in statistical language modeling. With almost one billion words of
	training data, we hope this benchmark will be useful to quickly evaluate
	novel language modeling techniques, and to compare their contribution
	when combined with other advanced techniques. We show performance
	of several well-known types of language models, with the best results
	achieved with a recurrent neural network based language model. The
	baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6;
	a combination of techniques leads to 35% reduction in perplexity,
	or 10% reduction in cross-entropy (bits), over that baseline. The
	benchmark is available as a code.google.com project; besides the
	scripts needed to rebuild the training/held-out data, it also makes
	available log-probability values for each word in each of ten held-out
	data sets, for each of the baseline n-gram models.},
  comments = {Accompanied by a code.google.com project allowing anyone to generate
	the benchmark data, and use it to compare their language model against
	the ones described in the paper},
  eprint = {1312.3005},
  file = {:oneBillionWordBenchmarkFormeasuringProgressInStatisticalLanguageModeling2014.pdf:PDF},
  oai2identifier = {1312.3005},
  owner = {rpickhardt},
  timestamp = {2014.05.06}
}

@ARTICLE{ChenGoodman1998empirical,
  author = {Chen, Stanly f. and Goodman, Joshua},
  title = {An Empirical study of smoothing Techniques for Language Modeling},
  year = {1998},
  file = {:ChenGoodman1998empirical.pdf:PDF},
  owner = {rpickhardt},
  timestamp = {2014.06.01}
}

@ARTICLE{gale1994wrong,
  author = {Gale, WA and Church, KW},
  title = {What is wrong with adding one},
  journal = {Corpus-based research into language},
  year = {1994},
  pages = {189--198},
  file = {:gale1994wrong.pdf:PDF}
}

@ARTICLE{goodman2001bit,
  author = {Goodman, Joshua},
  title = {A bit of progress in language modeling},
  journal = {Computer Speech \& Language},
  year = {2001},
  volume = {15},
  pages = {403--434},
  number = {4},
  file = {:goodman2001bit.pdf:PDF},
  owner = {rpickhardt},
  publisher = {Elsevier},
  timestamp = {2014.01.10}
}

@ARTICLE{J:TASSP:1987:Katz,
  author = {Katz, S.},
  title = {Estimation of probabilities from sparse data for the language model
	component of a speech recognizer},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  year = {1987},
  volume = {35},
  pages = {400-401},
  number = {3},
  abstract = {The description of a novel type of m-gram language model is given.
	The model offers, via a nonlinear recursive procedure, a computation
	and space efficient solution to the problem of estimating probabilities
	from sparse data. This solution compares favorably to other proposed
	methods. While the method has been developed for and successfully
	implemented in the IBM Real Time Speech Recognizers, its generality
	makes it applicable in other areas where the problem of estimating
	probabilities from sparse data arises.},
  doi = {10.1109/TASSP.1987.1165125},
  file = {:J\:TASSP\:1987\:Katz.pdf:PDF},
  issn = {0096-3518},
  keywords = {Acoustic signal processing;Maximum likelihood estimation;Natural languages;Probability;Recursive
	estimation;Speech processing;Speech recognition;Statistics},
  owner = {rpickhardt},
  timestamp = {2014.05.15}
}

@INPROCEEDINGS{P:ICASSP:1995:KneserN,
  author = {Kneser, Reinhard and Ney, Hermann},
  title = {Improved backing-off for m-gram language modeling},
  booktitle = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995
	International Conference on},
  year = {1995},
  volume = {1},
  pages = {181--184},
  organization = {IEEE},
  file = {:kneserNey1995.pdf:PDF},
  owner = {martin},
  timestamp = {2013.05.17}
}

@INCOLLECTION{RW:FST:2001,
  author = {Mihov, Stoyan and Maurel, Denis},
  title = {Direct construction of minimal acyclic subsequential transducers},
  booktitle = {Implementation and Application of Automata},
  publisher = {Springer},
  year = {2001},
  pages = {217--229},
  owner = {rpickhardt},
  timestamp = {2014.05.10}
}

@ARTICLE{shannon1951prediction,
  author = {Shannon, Claude E},
  title = {Prediction and entropy of printed English},
  journal = {Bell system technical journal},
  year = {1951},
  volume = {30},
  pages = {50--64},
  number = {1},
  file = {:shannon1951prediction.pdf:PDF},
  publisher = {Wiley Online Library}
}

@ARTICLE{shannon1949communication,
  author = {Shannon, Claude Elwood},
  title = {Communication in the presence of noise},
  journal = {Proceedings of the IRE},
  year = {1949},
  volume = {37},
  pages = {10--21},
  number = {1},
  file = {:shannon1949communication.pdf:PDF},
  publisher = {IEEE}
}

@ARTICLE{shannon2001mathematical,
  author = {Shannon, Claude Elwood},
  title = {A mathematical theory of communication},
  journal = {ACM SIGMOBILE Mobile Computing and Communications Review},
  year = {1948},
  volume = {5},
  pages = {3--55},
  number = {1},
  file = {:shannon2001mathematical.pdf:PDF},
  publisher = {ACM}
}

@ARTICLE{J:CSL:1999:ChenG,
  author = {Chen Stanley and Goodman Joshua},
  title = {An empirical study of smoothing techniques for language modeling},
  journal = {Computer Speech \& Language},
  year = {1999},
  volume = {13},
  pages = {359--393},
  number = {4},
  file = {:J\:CSL\:1999\:ChenG.pdf:PDF},
  owner = {martin},
  publisher = {Elsevier},
  timestamp = {2013.05.10}
}

@INPROCEEDINGS{Talbot07smoothedbloom,
  author = {David Talbot and Miles Osborne},
  title = {Smoothed bloom filter language models: Tera-scale LMs on the cheap},
  booktitle = {In Proc. of ACL},
  year = {2007},
  pages = {468--476},
  file = {:Talbot2007smoothedbloom.pdf:PDF},
  owner = {rpickhardt},
  timestamp = {2014.05.10}
}

@INPROCEEDINGS{Toutanova2003feature-richpart-of-speech,
  author = {Kristina Toutanova and Dan Klein and Christopher D. Manning and Yoram
	Singer},
  title = {Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network},
  booktitle = {IN PROCEEDINGS OF HLT-NAACL },
  year = {2003},
  pages = {252--259},
  file = {:Toutanova2003feature-richpart-of-speech.pdf:PDF},
  owner = {rpickhardt},
  timestamp = {2014.06.01}
}

